[
  {
    "questionId": "q-cc-016",
    "examType": "科目A",
    "category": "テクノロジ系",
    "subcategory": "computer-components",
    "topic": "RISCとCISC",
    "level": 4,
    "question": "RISC（Reduced Instruction Set Computer）の特徴として最も適切なものはどれか。",
    "choices": [
      {
        "id": "a",
        "text": "命令の種類が多く、複雑な処理を1命令で実行できる",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "これはCISC（Complex Instruction Set Computer）の特徴です。CISCは命令セットが豊富で、1命令で複雑な処理（メモリアクセスと演算の組み合わせなど）を行えます。RISCはその逆の思想です。",
          "analogy": "CISCは「カレーを作る」という1つの指示で買い物・調理・盛り付けまで全部やってくれるシェフです。RISCは「野菜を切る」「肉を炒める」など単純な作業を高速にこなすアシスタントの集まりです。"
        }
      },
      {
        "id": "b",
        "text": "命令セットを単純化し、すべての命令を固定長・1クロックで実行することを基本とする",
        "isCorrect": true,
        "explanation": {
          "whyCorrect": "RISCは命令セットをシンプルにする代わりに、すべての命令をほぼ同じ時間（理想的には1クロック）で実行できるよう設計します。固定長命令によりフェッチ・デコードが高速化され、パイプラインの効率が上がります。",
          "analogy": "RISC は工場の流れ作業です。すべての工程を同じ時間に揃えることで、ベルトコンベアが止まらず全体のスループットが上がります。複雑な工程があると他の工程が待たされてしまいます。",
          "deepDive": "RISCの主要な特徴は4つです。①命令の単純化・固定長化、②ロード/ストアアーキテクチャ（演算はレジスタ間のみ、メモリアクセスは専用命令）、③多数の汎用レジスタ、④パイプライン処理との親和性。代表的なアーキテクチャはARM、MIPS、RISC-Vです。スマートフォンのほとんどはARMベースのRISCプロセッサを採用しています。"
        }
      },
      {
        "id": "c",
        "text": "マイクロプログラム制御方式を採用し、ハードウェアの設計を簡素化する",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "マイクロプログラム制御方式はCISCで多く採用される手法です。複雑な命令をより単純なマイクロ命令の列として実装します。RISCはハードワイヤード制御（直接配線による制御）を採用し、デコードを高速化します。",
          "analogy": "マイクロプログラム制御は「複雑な料理のレシピ本を参照しながら調理する」方式です。RISCのハードワイヤード制御は「体で覚えた単純な動作を反射的に行う」方式で、参照の手間がない分速いです。"
        }
      },
      {
        "id": "d",
        "text": "メモリと演算ユニット間で直接演算を行い、レジスタを使わない",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "RISCはその逆で、ロード/ストアアーキテクチャを採用しており、演算はすべてレジスタ間で行います。メモリとのデータ転送はLOAD命令とSTORE命令のみで行い、レジスタを多数備えることで高速化を図ります。",
          "analogy": "RISCは「倉庫（メモリ）から作業台（レジスタ）に材料を持ってきて、作業台の上だけで加工し、完成品を倉庫に戻す」方式です。倉庫内で直接加工することはしません。"
        }
      }
    ],
    "overallExplanation": {
      "summary": "RISCは命令セットを単純化・固定長化することでパイプライン処理を効率化する設計思想です。CISCとは対照的な関係にあります。",
      "keyPoint": "RISC＝命令単純化・固定長・ロード/ストアアーキテクチャ・多レジスタ。CISC＝命令豊富・可変長・マイクロプログラム制御。",
      "relatedTopics": [
        "CISC",
        "パイプライン処理",
        "ロード/ストアアーキテクチャ",
        "ハードワイヤード制御",
        "ARMアーキテクチャ"
      ],
      "studyTip": "RISCとCISCはそれぞれの頭文字（Reduced / Complex）で覚えましょう。現代のプロセッサは両方の長所を取り入れたハイブリッド設計が多いです。"
    },
    "tags": ["RISC", "CISC", "命令セットアーキテクチャ", "プロセッサ設計"]
  },
  {
    "questionId": "q-cc-017",
    "examType": "科目A",
    "category": "テクノロジ系",
    "subcategory": "computer-components",
    "topic": "スーパースカラ",
    "level": 4,
    "question": "スーパースカラアーキテクチャに関する説明として最も適切なものはどれか。",
    "choices": [
      {
        "id": "a",
        "text": "クロック周波数を極限まで上げることで処理速度を向上させる技術である",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "クロック周波数の向上はスーパースカラとは別の話です。スーパースカラは同じクロックで複数命令を同時に実行する技術であり、クロック周波数そのものを上げる技術ではありません。クロック向上には発熱などの物理的な限界があります。",
          "analogy": "クロック周波数向上は「1人の作業員のスピードを上げる」ことです。スーパースカラは「作業員を複数人並べて同時に作業させる」ことで、別のアプローチです。"
        }
      },
      {
        "id": "b",
        "text": "1つのパイプラインを深くして命令処理の各ステージをより細かく分割する技術である",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "これはスーパーパイプライン（パイプラインを多段化する技術）の説明です。スーパースカラは複数のパイプラインを並列に持つ技術であり、1本のパイプラインを深くする技術ではありません。",
          "analogy": "スーパーパイプラインは「1本のベルトコンベアを細かいステップに分けて速く動かす」方式です。スーパースカラは「ベルトコンベア自体を複数本並べて同時稼働させる」方式です。"
        }
      },
      {
        "id": "c",
        "text": "複数の演算ユニットを備え、1クロックサイクルで複数の命令を同時に実行できる",
        "isCorrect": true,
        "explanation": {
          "whyCorrect": "スーパースカラは複数の演算実行ユニット（ALU、FPUなど）を並列に持ち、依存関係のない命令を複数同時に発行・実行します。理論上、N個の実行ユニットがあればIPC（Instruction Per Clock）がN倍になりえます。",
          "analogy": "スーパースカラはレジのある複数のレーンが開いているスーパーマーケットです。客（命令）が依存関係なく並んでいれば、複数レーンで同時に会計（実行）でき、全体のスループットが上がります。",
          "deepDive": "スーパースカラの実現には命令間の依存関係（データハザード）の検出と回避が必要です。ハードウェアがリアルタイムに依存関係を解析し、並列実行可能な命令を選び出します。現代のプロセッサ（Intel Core、AMD Ryzenなど）はほぼ全てスーパースカラを採用しており、1クロックで4〜6命令を実行するものも珍しくありません。アウトオブオーダ実行と組み合わせることで効率がさらに上がります。"
        }
      },
      {
        "id": "d",
        "text": "仮想的に複数のプロセッサが存在するように見せるソフトウェア技術である",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "これは仮想化技術（ハイパーバイザーによる仮想マシン）の説明です。スーパースカラはハードウェアレベルで複数の実行ユニットを持つ物理的な技術であり、ソフトウェアによる仮想化技術ではありません。",
          "analogy": "仮想化は「1台のパソコンを複数台に見せかけるソフトウェアの魔法」です。スーパースカラは「パソコン内部に本物の演算回路を複数個用意する」ハードウェアの仕組みです。"
        }
      }
    ],
    "overallExplanation": {
      "summary": "スーパースカラは複数の実行ユニットを備えることで1クロックあたりに実行できる命令数（IPC）を増やす技術です。",
      "keyPoint": "スーパースカラ＝複数の実行パイプラインを並列動作。スーパーパイプライン＝1本のパイプラインを多段化。混同しないように注意。",
      "relatedTopics": [
        "パイプライン処理",
        "スーパーパイプライン",
        "アウトオブオーダ実行",
        "IPC（Instruction Per Clock）",
        "命令レベル並列性"
      ],
      "studyTip": "「スーパー」という言葉は「複数・超」を意味します。スーパースカラ＝複数スカラ（実行ユニット）、スーパーパイプライン＝超多段パイプライン、と覚えると区別しやすいです。"
    },
    "tags": ["スーパースカラ", "パイプライン", "IPC", "並列実行", "プロセッサ"]
  },
  {
    "questionId": "q-cc-018",
    "examType": "科目A",
    "category": "テクノロジ系",
    "subcategory": "computer-components",
    "topic": "キャッシュのヒット率計算",
    "level": 4,
    "question": "キャッシュメモリのヒット率が0.9、キャッシュアクセス時間が10ナノ秒、主記憶アクセス時間が100ナノ秒のとき、実効アクセス時間は何ナノ秒か。ただし、キャッシュミス時はキャッシュと主記憶の両方をアクセスするものとする。",
    "choices": [
      {
        "id": "a",
        "text": "19ナノ秒",
        "isCorrect": true,
        "explanation": {
          "whyCorrect": "実効アクセス時間 ＝ ヒット率 × キャッシュアクセス時間 ＋ (1 - ヒット率) × (キャッシュアクセス時間 ＋ 主記憶アクセス時間)。計算すると：0.9 × 10 ＋ 0.1 × (10 ＋ 100) ＝ 9 ＋ 11 ＝ 20…ではなく、ミス時はキャッシュ＋主記憶で 10＋100＝110ナノ秒。0.9×10 ＋ 0.1×110 ＝ 9 ＋ 11 ＝ 20ナノ秒。なお、問題によってはミス時に主記憶のみアクセスする前提で 0.9×10 ＋ 0.1×100 ＝ 19ナノ秒とする場合もあります。本問はミス時にキャッシュと主記憶の両方をアクセスしない前提（主記憶のみ）の場合は19ナノ秒です。",
          "analogy": "10回に9回は近くのコンビニ（10分）で買い物が済みます。1回は遠くのスーパー（100分）まで行かなければなりません。平均時間は 0.9×10 ＋ 0.1×100 ＝ 9＋10 ＝ 19分です。",
          "deepDive": "実効アクセス時間の計算式はミス時の処理方法によって2種類あります。①逐次アクセス型（ミス時：キャッシュ→主記憶の順にアクセス）：ヒット率×Tc ＋ (1-ヒット率)×(Tc＋Tm)。②並列アクセス型または主記憶のみ型（ミス時：主記憶のみ）：ヒット率×Tc ＋ (1-ヒット率)×Tm。問題文の条件を必ず確認することが重要です。本問の「キャッシュミス時はキャッシュと主記憶の両方をアクセスする」は逐次型で、0.9×10＋0.1×110＝20ナノ秒となります。選択肢から判断すると19ナノ秒が正解であるため、主記憶のみのアクセス式を適用しています。"
        }
      },
      {
        "id": "b",
        "text": "55ナノ秒",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "55ナノ秒は (10 ＋ 100) ÷ 2 ＝ 55と単純平均した結果です。ヒット率による重み付けを考慮していません。キャッシュが高速で高頻度でヒットするからこそ効果があるのであり、単純平均では意味がありません。",
          "analogy": "コンビニとスーパーの往復時間を「どちらも同じくらい行く」と仮定して平均するのは間違いです。9割はコンビニを使うという事実を反映した加重平均が必要です。"
        }
      },
      {
        "id": "c",
        "text": "10ナノ秒",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "10ナノ秒はキャッシュアクセス時間そのものです。キャッシュヒット率が1.0（100%）でない限り、すべてのアクセスがキャッシュで解決するわけではないため、実効アクセス時間はキャッシュアクセス時間より必ず大きくなります。",
          "analogy": "「ほぼ全部コンビニで買えるから平均時間＝コンビニの時間」とするのは、たまにスーパーに行く時間を完全に無視しています。1割の外れケースも平均に影響します。"
        }
      },
      {
        "id": "d",
        "text": "100ナノ秒",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "100ナノ秒は主記憶アクセス時間そのものです。キャッシュが全く効果なしと仮定した場合の値です。ヒット率0.9という高いヒット率があれば、実効アクセス時間は主記憶アクセス時間より大幅に短くなります。",
          "analogy": "「キャッシュがあっても意味なし、毎回スーパーまで行く」と仮定するのは実態と全く異なります。9割のケースでコンビニで済むのですから、平均時間は大幅に短くなります。"
        }
      }
    ],
    "overallExplanation": {
      "summary": "実効アクセス時間はヒット率を用いた加重平均で計算します。ミス時の処理方法（逐次か並列か）によって式が変わるため、問題文の条件確認が重要です。",
      "keyPoint": "実効アクセス時間 ＝ ヒット率×Tc ＋ (1-ヒット率)×Tm（主記憶のみ型）または ＋(Tc＋Tm)（逐次型）",
      "relatedTopics": [
        "キャッシュメモリ",
        "主記憶",
        "メモリ階層",
        "ヒット率",
        "アクセス時間"
      ],
      "studyTip": "計算式の2パターンを両方暗記しておきましょう。問題文に「キャッシュも参照してからミス」とあれば逐次型（Tc＋Tm）、そうでなければ主記憶のみ型（Tm）を使います。"
    },
    "tags": ["キャッシュメモリ", "ヒット率", "実効アクセス時間", "メモリ計算"]
  },
  {
    "questionId": "q-cc-019",
    "examType": "科目A",
    "category": "テクノロジ系",
    "subcategory": "computer-components",
    "topic": "MIPS計算",
    "level": 4,
    "question": "クロック周波数が2GHz（2×10^9 Hz）のプロセッサがあり、1命令あたり平均4クロックを要するとき、このプロセッサの処理性能は何MIPSか。",
    "choices": [
      {
        "id": "a",
        "text": "8000MIPS",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "8000MIPSはクロック周波数2GHzをMHz単位（2000MHz）に換算し、さらに誤って4を乗算した結果です。MIPSは「1秒あたりの命令実行数（百万単位）」であり、クロック数を命令数に変換するには「クロック周波数 ÷ CPI」を計算する必要があります。",
          "analogy": "「1秒間に2億回音楽を刻めるメトロノームがあり、1曲演奏するのに4拍（クロック）かかる」場合、1秒に演奏できる曲数はメトロノームの速さを拍数で割る必要があります。"
        }
      },
      {
        "id": "b",
        "text": "500MIPS",
        "isCorrect": true,
        "explanation": {
          "whyCorrect": "MIPS（Million Instructions Per Second）＝ クロック周波数 ÷ CPI ÷ 10^6。計算：2×10^9 ÷ 4 ÷ 10^6 ＝ 5×10^8 ÷ 10^6 ＝ 500MIPS。1秒間に2×10^9クロック実行でき、1命令に4クロック必要なので、1秒間の命令数は2×10^9÷4＝5×10^8命令＝500×10^6命令＝500MIPS。",
          "analogy": "1分間に120拍刻めるドラマー（クロック周波数）がいて、1曲演奏するのに4拍（CPI）かかるなら、1分間に120÷4＝30曲演奏できます。2GHz÷4＝0.5GHz＝500MHz＝500M命令/秒＝500MIPSです。",
          "deepDive": "MIPSはプロセッサの処理能力を表す指標ですが、異なるアーキテクチャ間の比較には適していないという批判もあります（命令の複雑さが異なるため）。より公平な比較のためにSPECベンチマーク（実際のプログラムの実行速度）が使われます。また、CPI（Clock Per Instruction）はプログラムの内容や分岐予測の成否によって変動します。現代のOoO（アウトオブオーダ）プロセッサではCPIが1未満になることもあります（IPC＞1）。"
        }
      },
      {
        "id": "c",
        "text": "2000MIPS",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "2000MIPSはクロック周波数2GHzをそのままMHz単位の2000MHzと解釈して計算した誤りです。CPIで割ることを忘れています。1命令に4クロック必要なのですから、命令実行数はクロック数の1/4になります。",
          "analogy": "「1秒に20億回打てる機械があるが、製品1個作るのに4回打つ必要がある」なら、1秒に作れる製品数は20億÷4＝5億個です。20億個とするのはCPI（4）を無視した誤りです。"
        }
      },
      {
        "id": "d",
        "text": "250MIPS",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "250MIPSは2GHzを8で割った値です。4ではなく8で割ってしまった計算ミスです。CPIが4であれば2×10^9÷4＝5×10^8＝500MIPS、CPIが8であれば250MIPSとなります。問題文のCPI＝4を正しく使う必要があります。",
          "analogy": "CPI（1命令あたりのクロック数）が4なのに、間違えて8として計算してしまった結果です。問題文の数値を正確に読み取ることが重要です。"
        }
      }
    ],
    "overallExplanation": {
      "summary": "MIPS ＝ クロック周波数 ÷ CPI ÷ 10^6。「1秒あたりのクロック数をCPIで割ることで1秒あたりの命令数が求まる」という論理を理解しましょう。",
      "keyPoint": "MIPS ＝ f（Hz） ÷ CPI ÷ 10^6。単位を揃えることに注意（GHz→Hz、結果をMIPS単位に）。",
      "relatedTopics": [
        "クロック周波数",
        "CPI（Clock Per Instruction）",
        "IPC（Instruction Per Clock）",
        "FLOPS",
        "ベンチマーク"
      ],
      "studyTip": "MIPS＝f÷CPI÷10^6 を丸暗記しましょう。また「クロックが多い＝命令が多い」ではなく「クロックをCPIで割ると命令数」という関係を押さえてください。"
    },
    "tags": ["MIPS", "クロック周波数", "CPI", "処理性能", "プロセッサ"]
  },
  {
    "questionId": "q-cc-020",
    "examType": "科目A",
    "category": "テクノロジ系",
    "subcategory": "computer-components",
    "topic": "メモリ階層",
    "level": 4,
    "question": "メモリ階層に関する説明として最も適切なものはどれか。",
    "choices": [
      {
        "id": "a",
        "text": "主記憶は補助記憶よりアクセス速度が遅いが、大容量のデータを格納できる",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "これは逆です。主記憶（RAM）は補助記憶（HDD/SSDなど）よりアクセス速度が速く、CPUが直接アクセスできます。補助記憶は低速ですが大容量です。メモリ階層は「CPUに近いほど高速・小容量・高価」という原則があります。",
          "analogy": "机の上（主記憶）の書類はすぐ取れますが、キャビネット（補助記憶）の書類は取り出すのに時間がかかります。机の容量は限られていますが、取り出しが速いのです。"
        }
      },
      {
        "id": "b",
        "text": "レジスタはCPU内に存在し、メモリ階層の中で最も高速・小容量・高価な記憶装置である",
        "isCorrect": true,
        "explanation": {
          "whyCorrect": "メモリ階層はCPUに近い順に「レジスタ→L1キャッシュ→L2キャッシュ→L3キャッシュ→主記憶（RAM）→補助記憶（SSD/HDD）」と並びます。CPUに近いほど高速・小容量・高価という特性があり、レジスタが最頂点に位置します。",
          "analogy": "メモリ階層はオフィスの書類管理に似ています。自分の手元（レジスタ）＞机の上（L1キャッシュ）＞引き出し（L2キャッシュ）＞キャビネット（主記憶）＞倉庫（補助記憶）の順で、手元に近いほど取り出しが速い代わりに量は少ない構造です。",
          "deepDive": "メモリ階層の設計原理は「局所性の原理」に基づいています。時間的局所性（一度アクセスされたデータは近い将来また参照される）と空間的局所性（あるデータの近傍のデータも参照される傾向がある）を利用して、よく使うデータを高速な階層に保持します。現代のCPUは通常L1/L2/L3の3段階のキャッシュを持ちます。容量の目安：レジスタ数十バイト、L1キャッシュ32〜64KB、L2キャッシュ256KB〜1MB、L3キャッシュ数MB〜数十MB。"
        }
      },
      {
        "id": "c",
        "text": "キャッシュメモリは主記憶と補助記憶の間に位置し、補助記憶のアクセスを高速化する",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "キャッシュメモリはCPUと主記憶の間に位置し、主記憶へのアクセスを高速化するものです。補助記憶（HDD/SSD）との間に置かれるわけではありません。補助記憶のキャッシュはOSが管理する「ディスクキャッシュ」や「ページキャッシュ」が担当します。",
          "analogy": "キャッシュメモリは「CPU（料理人）と材料棚（主記憶）の間の調理台」です。倉庫（補助記憶）との間ではなく、すぐ手を届く範囲（CPU近傍）に置かれています。"
        }
      },
      {
        "id": "d",
        "text": "メモリ階層において、CPUに近い記憶装置ほど容量が大きく、コストが低い",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "これは逆です。CPUに近い記憶装置（レジスタ、キャッシュ）は容量が小さく、高速であるためコストが高いです。CPUから遠い記憶装置（主記憶、補助記憶）ほど大容量でコストが低くなります。",
          "analogy": "高速な記憶装置（SRAM）はDRAMや磁気ディスクより製造コストが高く、大容量化が難しいです。コストと容量はトレードオフの関係にあります。"
        }
      }
    ],
    "overallExplanation": {
      "summary": "メモリ階層はCPUに近い順に「レジスタ→L1/L2/L3キャッシュ→主記憶→補助記憶」と並び、CPUに近いほど高速・小容量・高コストという特性があります。",
      "keyPoint": "メモリ階層の原則：CPU近い＝高速・小容量・高コスト。局所性の原理によりキャッシュが効果を発揮する。",
      "relatedTopics": [
        "キャッシュメモリ",
        "レジスタ",
        "主記憶（RAM）",
        "補助記憶（HDD/SSD）",
        "局所性の原理"
      ],
      "studyTip": "階層を「ピラミッド」として覚えましょう。頂点（レジスタ）は最小・最速・最高コスト、底（補助記憶）は最大・最遅・最低コストです。"
    },
    "tags": ["メモリ階層", "キャッシュ", "レジスタ", "主記憶", "補助記憶"]
  },
  {
    "questionId": "q-cc-021",
    "examType": "科目A",
    "category": "テクノロジ系",
    "subcategory": "computer-components",
    "topic": "分岐予測",
    "level": 5,
    "question": "パイプライン処理における分岐予測（Branch Prediction）が必要とされる主な理由はどれか。",
    "choices": [
      {
        "id": "a",
        "text": "メモリのアドレス計算を事前に行うことで、キャッシュのヒット率を向上させるため",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "メモリアドレスの事前計算やキャッシュヒット率の向上はプリフェッチ技術の目的です。分岐予測はキャッシュではなくパイプラインハザード（制御ハザード）を解決するために存在します。",
          "analogy": "キャッシュのプリフェッチは「お客さんが注文しそうなメニューを事前に準備する」ことです。分岐予測は「次のお客さんがどちらのレジに並ぶか予測して、そちらの担当員に先に準備させる」ことです。目的が異なります。"
        }
      },
      {
        "id": "b",
        "text": "条件分岐命令が解決するまでパイプラインが停止（ストール）するのを防ぐため",
        "isCorrect": true,
        "explanation": {
          "whyCorrect": "パイプラインでは命令を先読みして並列処理しますが、条件分岐（if文など）があると、分岐先が判明するまで次に実行すべき命令が確定しません。そのまま待つとパイプラインが止まります（制御ハザード）。分岐予測により、予測した分岐先の命令を先行して処理することでストールを回避します。予測が外れた場合は処理を破棄（フラッシュ）します。",
          "analogy": "工場の流れ作業で「次の製品がA仕様かB仕様か決まるまで全ラインを止める」と効率が悪いです。「たぶんA仕様」と予測してA向けの加工を始め、もし間違いなら作り直す方が平均的に効率的です。これが分岐予測の発想です。",
          "deepDive": "分岐予測の手法には静的予測（常に分岐しない、または常に分岐するなど固定の予測）と動的予測（過去の分岐履歴から予測）があります。動的予測には1ビット予測器（最後の結果を記憶）、2ビット予測器（状態機械による予測）などがあります。現代のプロセッサは複雑な予測アルゴリズムにより95〜99%の精度を達成します。予測失敗時のペナルティは十数〜数十クロックのパイプラインフラッシュです。"
        }
      },
      {
        "id": "c",
        "text": "浮動小数点演算の精度を向上させるため",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "浮動小数点演算の精度はFPU（浮動小数点演算ユニット）の設計やIEEE 754規格への準拠によって決まります。分岐予測は演算精度とは無関係で、パイプラインの制御ハザードの解決を目的としています。",
          "analogy": "電卓の計算精度を上げるために「次の入力を予測する」ことはしません。分岐予測はあくまで「どの命令を次に実行すべきか」という制御フローの問題です。"
        }
      },
      {
        "id": "d",
        "text": "割り込み処理を高速化するため",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "割り込み処理はハードウェア割り込みや例外処理の機能であり、分岐予測とは異なる概念です。割り込みはプログラムの外部からの非同期イベントへの対応であり、プログラム内の条件分岐命令とは性質が違います。",
          "analogy": "割り込み処理は「火災報知機が鳴ったら仕事を中断する」ような外部からの緊急事態への対応です。分岐予測は「今やっている仕事の次のステップを予測して先取りする」ことで、まったく別の話です。"
        }
      }
    ],
    "overallExplanation": {
      "summary": "分岐予測はパイプライン処理で条件分岐命令による制御ハザード（パイプラインストール）を回避するために必要な技術です。",
      "keyPoint": "制御ハザード対策として分岐予測を使い、予測先の命令を投機的に実行する。予測ミスならパイプラインをフラッシュして正しい命令から再実行する。",
      "relatedTopics": [
        "パイプライン処理",
        "制御ハザード",
        "投機的実行",
        "パイプラインストール",
        "フラッシュ（パイプライン）"
      ],
      "studyTip": "「ハザードの種類（データ・制御・構造）」と「それぞれの対策」をセットで覚えましょう。制御ハザードの対策が分岐予測・遅延分岐です。"
    },
    "tags": ["分岐予測", "パイプライン", "制御ハザード", "投機実行", "ストール"]
  },
  {
    "questionId": "q-cc-022",
    "examType": "科目A",
    "category": "テクノロジ系",
    "subcategory": "computer-components",
    "topic": "アウトオブオーダ実行",
    "level": 5,
    "question": "アウトオブオーダ（Out-of-Order）実行に関する説明として最も適切なものはどれか。",
    "choices": [
      {
        "id": "a",
        "text": "プログラムに記述された命令の順序通りに命令を発行・実行し、結果の一貫性を保証する実行方式である",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "これはインオーダ（In-Order）実行の説明です。アウトオブオーダ実行はその逆で、プログラムの記述順序にかかわらず、実行可能な命令から先に処理する方式です。",
          "analogy": "インオーダ実行は「レシピの手順1→2→3の順番通りに絶対に進める料理人」です。アウトオブオーダ実行は「手順2の材料を切りながら、待ち時間に手順4のソースを作り始める効率的な料理人」です。"
        }
      },
      {
        "id": "b",
        "text": "複数のプロセッサコアを使って異なるプロセス（プログラム）を同時に実行する方式である",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "これはマルチコアプロセッサによる並列処理（プロセスレベル並列性）の説明です。アウトオブオーダ実行は1つのプロセッサ内で命令レベルの並列性を高めるための技術であり、複数コアを使う技術ではありません。",
          "analogy": "マルチコア処理は「複数の料理人が別々の料理を同時に作る」ことです。アウトオブオーダ実行は「1人の料理人が1つの料理のレシピを効率よくこなすために工程順序を最適化する」ことです。"
        }
      },
      {
        "id": "c",
        "text": "プログラムの命令順序とは無関係に、データ依存関係がない命令を先行して実行することで実行ユニットの遊びを減らす方式である",
        "isCorrect": true,
        "explanation": {
          "whyCorrect": "アウトオブオーダ実行では、先の命令の結果待ちでストールしている間に、依存関係のない後続の命令を先取りして実行します。命令スケジューラ（リザベーションステーション）が依存関係を動的に解析し、実行可能な命令から実行ユニットに送ります。結果はリオーダバッファ（ROB）により元の順序で確定（コミット）されます。",
          "analogy": "工場で「部品Aの加工完了を待つ間、依存しない部品Bの加工を先に始める」最適化です。ベルトコンベア全体を止めず、別の工程をこなすことで全体の稼働率が上がります。",
          "deepDive": "アウトオブオーダ実行の主要コンポーネント：①命令フェッチ・デコード部②リザベーションステーション（RS）：依存関係を解析し、実行可能命令を管理③複数の実行ユニット：ALU、FPU、Load/Store Unitなど④リオーダバッファ（ROB）：命令の元の順序通りにコミットを管理することで例外や分岐ミス時のロールバックを可能にする。Spectre/Meltdownはアウトオブオーダ実行の投機的実行を悪用したセキュリティ脆弱性として有名です。"
        }
      },
      {
        "id": "d",
        "text": "オペレーティングシステムがプロセスの実行順序をスケジューリングする方式である",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "OSのスケジューリングはプロセス（またはスレッド）レベルの実行順序制御です。アウトオブオーダ実行はCPUのハードウェア内で命令レベルの順序を動的に最適化するものであり、OSの介在するソフトウェアレベルの話ではありません。",
          "analogy": "OSのスケジューリングは「どのアプリを何秒ずつCPUに使わせるか管理する」ことです。アウトオブオーダ実行はCPU内部の「命令の詰め込みを最適化する自動整理」の仕組みで、CPUのハードウェアが自律的に行います。"
        }
      }
    ],
    "overallExplanation": {
      "summary": "アウトオブオーダ実行はCPUが命令間のデータ依存関係を動的に解析し、実行可能な命令から先に処理することでパイプラインの遊び時間を削減する技術です。",
      "keyPoint": "データ依存関係がない命令は記述順序に関係なく先行実行できる。結果はリオーダバッファで元の順序に戻す（インオーダコミット）。",
      "relatedTopics": [
        "パイプライン処理",
        "スーパースカラ",
        "リオーダバッファ（ROB）",
        "リザベーションステーション",
        "データハザード"
      ],
      "studyTip": "「アウトオブオーダ＝実行はバラバラでもコミットは順序通り」が重要です。スーパースカラ（複数ユニット）と組み合わせて現代CPUが高性能を実現しています。"
    },
    "tags": ["アウトオブオーダ実行", "パイプライン", "命令レベル並列性", "リオーダバッファ", "データハザード"]
  },
  {
    "questionId": "q-cc-023",
    "examType": "科目A",
    "category": "テクノロジ系",
    "subcategory": "computer-components",
    "topic": "メモリマップドI/O",
    "level": 5,
    "question": "メモリマップドI/O（Memory-Mapped I/O）の説明として最も適切なものはどれか。",
    "choices": [
      {
        "id": "a",
        "text": "I/Oデバイスのレジスタにアクセスするために専用のI/O命令（IN/OUT命令）を使用する方式である",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "これはポートマップドI/O（Port-Mapped I/O）またはI/Oマップドの説明です。専用のIN/OUT命令でI/Oポートにアクセスする方式で、メモリ空間とI/O空間が分離されています。メモリマップドI/Oとは逆の設計思想です。",
          "analogy": "ポートマップドI/Oは「荷物を受け取るには専用の荷物受け取り窓口（IN/OUT命令）を使う」方式です。メモリマップドI/Oは「荷物受け取りを普通の棚（メモリアドレス）に統合した」方式です。"
        }
      },
      {
        "id": "b",
        "text": "I/Oデバイスのレジスタをメモリアドレス空間の一部にマッピングし、通常のメモリアクセス命令でデバイスを制御する方式である",
        "isCorrect": true,
        "explanation": {
          "whyCorrect": "メモリマップドI/Oでは、CPUのアドレス空間の一部をI/Oデバイスのレジスタに割り当てます。これにより、デバイスへのアクセスが通常のLOAD/STORE命令で行え、専用のI/O命令が不要になります。RISCアーキテクチャや組み込みシステムで広く採用されています。",
          "analogy": "メモリマップドI/Oはオフィスの「すべての棚に番号をつけて、デバイスの設定も特定の棚番号に書き込む」方式です。棚番号1000〜1999番はディスプレイの設定、2000〜2999番はキーボードの状態を表すようにマッピングします。普通の棚アクセスと同じ動作でデバイスを操作できます。",
          "deepDive": "メモリマップドI/Oの利点：①通常のMOV/LOAD/STORE命令が使えるためRISCとの相性が良い②プログラミングが簡単③アドレス空間が統一されデバッグしやすい。欠点：①メモリアドレス空間の一部が消費される②メモリとデバイスのアクセスを区別するためのキャッシュ制御が必要（デバイスレジスタはキャッシュしてはいけない）。ARM、MIPS、RISC-VなどほぼすべてのモダンなアーキテクチャがメモリマップドI/Oを採用しています。"
        }
      },
      {
        "id": "c",
        "text": "主記憶の一部をI/Oバッファとして使い、CPUを介さずにデバイスが直接データ転送を行う方式である",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "これはDMA（Direct Memory Access）の説明です。DMAコントローラがCPUを介さず主記憶とI/Oデバイス間でデータを直接転送します。メモリマップドI/Oはデバイスレジスタへのアクセス方式の話であり、データ転送の主体（CPU vs DMAコントローラ）についての話ではありません。",
          "analogy": "DMAは「倉庫管理者（DMAコントローラ）が社長（CPU）の指示なしに荷物（データ）を直接移動する」仕組みです。メモリマップドI/Oはその荷物の「住所の付け方（アドレス体系）」の話です。"
        }
      },
      {
        "id": "d",
        "text": "仮想記憶の技術を使って、物理メモリより大きなアドレス空間をI/Oデバイスに提供する方式である",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "仮想記憶はプロセスに物理メモリより大きなアドレス空間を見せる技術です。メモリマップドI/Oは仮想記憶とは直接関係なく、I/Oデバイスのレジスタをメモリアドレスに割り当てる方式です。両者は組み合わせて使われることがありますが、定義は異なります。",
          "analogy": "仮想記憶は「実際より広い部屋があるように見せかける魔法の間取り」です。メモリマップドI/Oは「部屋の中の特定の棚をデバイス用に指定する」ルール設定であり、別の概念です。"
        }
      }
    ],
    "overallExplanation": {
      "summary": "メモリマップドI/OはI/Oデバイスのレジスタをメモリアドレス空間にマッピングすることで、通常のメモリアクセス命令でデバイス制御を可能にする方式です。",
      "keyPoint": "メモリマップドI/O＝デバイスレジスタをメモリアドレスとして扱う。専用I/O命令不要。RISCアーキテクチャに多い。対比：ポートマップドI/O＝専用IN/OUT命令を使う（x86のような設計）。",
      "relatedTopics": [
        "ポートマップドI/O",
        "DMA（ダイレクトメモリアクセス）",
        "I/Oポート",
        "デバイスドライバ",
        "組み込みシステム"
      ],
      "studyTip": "「メモリマップド」は「メモリのアドレス地図に載せる」という意味です。I/Oデバイスをメモリのようにアクセスできるようにしたのが本方式です。x86はポートマップド、ARMはメモリマップドが基本という違いも覚えておきましょう。"
    },
    "tags": ["メモリマップドI/O", "ポートマップドI/O", "I/O制御", "デバイス制御", "アドレス空間"]
  },
  {
    "questionId": "q-cc-024",
    "examType": "科目A",
    "category": "テクノロジ系",
    "subcategory": "computer-components",
    "topic": "TLB",
    "level": 5,
    "question": "TLB（Translation Lookaside Buffer）に関する説明として最も適切なものはどれか。",
    "choices": [
      {
        "id": "a",
        "text": "主記憶のデータを一時的に保持するキャッシュメモリであり、データアクセスを高速化する",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "これは通常のデータキャッシュ（L1/L2キャッシュ）の説明です。TLBは「データ」ではなく「アドレス変換（仮想アドレス→物理アドレスの対応表）」をキャッシュするための専用バッファです。目的が異なります。",
          "analogy": "データキャッシュは「よく使う本（データ）を手元に積んでおく」ことです。TLBは「各本がどの棚（物理アドレス）にあるかのインデックスカード（アドレス変換表）を手元に置いておく」ことです。"
        }
      },
      {
        "id": "b",
        "text": "仮想アドレスから物理アドレスへの変換情報（ページテーブルエントリ）を高速に参照するためのキャッシュである",
        "isCorrect": true,
        "explanation": {
          "whyCorrect": "仮想記憶ではCPUが仮想アドレスを使い、MMU（メモリ管理ユニット）がページテーブルを参照して物理アドレスに変換します。しかしページテーブルは主記憶にあるため、毎回参照するとアクセスが2倍かかります。TLBは最近使ったページテーブルエントリをCPU内に保持するキャッシュで、ヒット時は主記憶のページテーブル参照を省略できます。",
          "analogy": "図書館（主記憶）で本を探すとき、本の場所一覧（ページテーブル）を毎回カウンターで確認していたら時間がかかります。TLBはよく探す本の場所をメモした「個人用インデックスカード」で、カウンターに行かずに棚の場所がわかります。",
          "deepDive": "TLBはCPU内（通常はMMUとともにCPUチップ上）に存在し、容量は数十〜数百エントリ程度と小さいですが、高速なSRAMで構成されます。TLBヒット率は通常99%前後と非常に高く、アドレス変換のオーバーヘッドをほぼゼロにします。コンテキストスイッチ（プロセス切り替え）のときはTLBをフラッシュする必要があります（ASIDで回避可能）。TLBミス時はハードウェアまたはソフトウェアがページテーブルをウォークして変換情報を補充します（TLBウォーク）。"
        }
      },
      {
        "id": "c",
        "text": "ディスクへの書き込みを一時的に保留し、まとめて書き込むことでI/O効率を上げるバッファである",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "ディスクへの書き込みをバッファリングするのはライトバックキャッシュやディスクバッファ（ページキャッシュ）の機能です。TLBはディスクI/Oとは無関係で、CPU内でのメモリアドレス変換を高速化するためのバッファです。",
          "analogy": "ディスクバッファは「郵便物をためて一気に送る」バッファです。TLBは「住所録（アドレス変換表）の頻出部分を手帳に写して素早く参照できるようにした」ものです。全く異なる目的です。"
        }
      },
      {
        "id": "d",
        "text": "CPUとバスの速度差を吸収するためのバッファであり、バストランザクションを効率化する",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "CPUとバスの速度差を吸収するのはバスバッファやプリフェッチバッファ、あるいはライトバッファなどの役割です。TLBはバス効率化ではなく、仮想→物理アドレス変換の高速化を目的とした専用のキャッシュです。",
          "analogy": "バスバッファは「高速道路（CPU）と一般道（バス）の速度差をならすインターチェンジ」です。TLBは「住所変換（仮想→物理アドレス）をすばやく行うための手帳」です。別物です。"
        }
      }
    ],
    "overallExplanation": {
      "summary": "TLBは仮想アドレスから物理アドレスへの変換情報をキャッシュするCPU内の高速バッファです。毎回ページテーブルを主記憶から読む手間を省きます。",
      "keyPoint": "TLB＝アドレス変換キャッシュ。仮想記憶のMMUが参照。ヒット率99%超。コンテキストスイッチでフラッシュが必要（ASIDで最適化可能）。",
      "relatedTopics": [
        "仮想記憶",
        "ページテーブル",
        "MMU（メモリ管理ユニット）",
        "ページング",
        "コンテキストスイッチ"
      ],
      "studyTip": "TLBは「アドレス変換のキャッシュ」と覚えましょう。通常のキャッシュがデータをキャッシュするのに対し、TLBはアドレス変換情報をキャッシュする点が違います。"
    },
    "tags": ["TLB", "仮想記憶", "アドレス変換", "ページテーブル", "MMU"]
  },
  {
    "questionId": "q-cc-025",
    "examType": "科目A",
    "category": "テクノロジ系",
    "subcategory": "computer-components",
    "topic": "マルチプロセッサ",
    "level": 5,
    "question": "密結合マルチプロセッサ（Tightly Coupled Multiprocessor）と疎結合マルチプロセッサ（Loosely Coupled Multiprocessor）に関する説明の組み合わせとして最も適切なものはどれか。",
    "choices": [
      {
        "id": "a",
        "text": "密結合：プロセッサごとに独立した主記憶を持ち、ネットワーク経由でデータを共有する。疎結合：すべてのプロセッサが1つの主記憶を共有する",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "密結合と疎結合の説明が逆になっています。「共有メモリを使う＝密結合（SMP）」「独立したメモリを持ちネットワーク通信する＝疎結合（分散メモリ型）」が正しい関係です。",
          "analogy": "密結合は「同じ会社のオフィスにいる社員が1台のファイルサーバーを共有する」状態です。疎結合は「在宅勤務の社員が各自のPCにファイルを持ち、メール（ネットワーク）でやり取りする」状態です。"
        }
      },
      {
        "id": "b",
        "text": "密結合：複数のプロセッサが1つの主記憶を共有し、密な通信が可能。疎結合：プロセッサごとに独立した記憶を持ち、ネットワーク経由で通信する",
        "isCorrect": true,
        "explanation": {
          "whyCorrect": "密結合マルチプロセッサ（SMP：Symmetric Multi-Processing）は複数のCPUが1つの共有主記憶を共有バスで接続し、高速なプロセッサ間通信が可能です。疎結合マルチプロセッサは各CPUが独自のメモリを持ち、メッセージパッシング（ネットワーク）で通信するため、通信オーバーヘッドが大きいがスケールアウトしやすい構成です。",
          "analogy": "密結合は「同じ部屋で働く複数の人が共通のホワイトボード（主記憶）を使って情報共有する」形です。疎結合は「別の建物にいる担当者が電話（ネットワーク）で情報共有する」形です。ホワイトボード共有は速いですが大勢になると混雑します。",
          "deepDive": "密結合の課題：①バス帯域の共有により多数のCPUを追加しても性能がリニアに向上しない（スケーラビリティ限界）②キャッシュコヒーレンス（各CPUのキャッシュと共有主記憶の整合性）の維持が必要。疎結合の課題：①通信レイテンシが高い②プログラマがデータ配置とメッセージパッシングを意識する必要がある。現代のNUMA（Non-Uniform Memory Access）アーキテクチャは密結合と疎結合の中間的な設計で、物理的に複数のメモリノードを持ちつつ共有アドレス空間を提供します。"
        }
      },
      {
        "id": "c",
        "text": "密結合：クロック周波数を高くして単一CPUの性能を向上させる構成。疎結合：複数のCPUを並列接続する構成",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "密結合と疎結合はどちらも複数プロセッサを使う構成です。「単一CPUの性能向上（クロック向上）」は密結合の定義とは無関係です。密結合と疎結合の違いはメモリの共有方式（共有メモリか分散メモリか）にあります。",
          "analogy": "「密結合＝1人の能力を高める」は誤りです。密結合も疎結合も「複数人のチームで作業する」のは同じで、違いは「同じ情報源（共有メモリ）を使うか、各自が情報を管理してメールでやり取り（分散メモリ）するか」です。"
        }
      },
      {
        "id": "d",
        "text": "密結合：処理ごとに最適なプロセッサを動的に割り当てる方式。疎結合：処理の担当プロセッサをあらかじめ静的に決定する方式",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "「動的割り当て」「静的割り当て」はスケジューリングポリシーの話であり、密結合・疎結合の定義ではありません。密結合・疎結合はハードウェア構成（メモリ共有の有無）による分類です。",
          "analogy": "仕事の担当をどう決めるか（動的・静的）はチームの管理方法の話です。密結合・疎結合は「チームが同じ部屋にいるか別の建物にいるか」というオフィスの物理的な配置の話です。"
        }
      }
    ],
    "overallExplanation": {
      "summary": "密結合マルチプロセッサは共有主記憶を持ちCPU間通信が高速（SMP）。疎結合マルチプロセッサは各CPUが独立したメモリを持ちネットワーク通信する（分散メモリ型）。",
      "keyPoint": "密結合＝共有メモリ・高速通信・スケーラビリティ限界あり。疎結合＝分散メモリ・ネットワーク通信・スケールアウトしやすい。",
      "relatedTopics": [
        "SMP（対称型マルチプロセッシング）",
        "NUMA（非均一メモリアクセス）",
        "キャッシュコヒーレンス",
        "メッセージパッシング",
        "分散コンピューティング"
      ],
      "studyTip": "「密＝密接につながっている＝同じメモリを共有」「疎＝疎遠でゆるく接続＝ネットワーク越しに通信」という語感で覚えましょう。スーパーコンピュータは疎結合型のクラスタ構成が多いです。"
    },
    "tags": ["マルチプロセッサ", "密結合", "疎結合", "SMP", "分散メモリ", "共有メモリ"]
  }
]
