[
  {
    "questionId": "q-cc-026",
    "examType": "科目A",
    "category": "テクノロジ系",
    "subcategory": "computer-components",
    "topic": "命令パイプラインのハザード",
    "level": 6,
    "question": "パイプラインにおける「データハザード」の説明として最も適切なものはどれか。",
    "choices": [
      {
        "id": "a",
        "text": "条件分岐命令により、次に実行すべき命令が確定するまでパイプラインを一時停止しなければならない状況。",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "これは「制御ハザード（分岐ハザード）」の説明です。分岐命令の行き先が確定するまでパイプラインが止まる問題を指します。",
          "analogy": "交差点で「右か左か」を決めかねてクルマが止まる状態が制御ハザードです。データハザードは別の原因で詰まります。"
        }
      },
      {
        "id": "b",
        "text": "前の命令が書き込んだ結果を、後続の命令が読み込もうとするとき、まだ結果が確定していないために生じる停止。",
        "isCorrect": true,
        "explanation": {
          "whyCorrect": "データハザードは「RAW（Read After Write）依存」が代表例です。命令Aがレジスタに値を書く前に、命令Bがそのレジスタを読もうとすると値が間に合わず、パイプラインにバブル（無駄なサイクル）が挿入されます。フォワーディング（バイパス）技術で一部緩和できます。",
          "analogy": "工場の流れ作業で、前の工程が部品を完成させる前に次の工程が取りに来てしまう状態です。部品がまだできていないので後工程は待つしかありません。",
          "deepDive": "データ依存には RAW（Read After Write）・WAR（Write After Read）・WAW（Write After Write）の3種があります。パイプラインで問題になるのは主に RAW です。対策としてフォワーディング（演算結果を次の命令に直結する回路）や命令スケジューリング（コンパイラが命令順を入れ替えて依存を回避）があります。"
        }
      },
      {
        "id": "c",
        "text": "複数の命令が同じメモリバスやキャッシュポートへ同時にアクセスしようとして生じる競合。",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "これは「構造ハザード（リソースハザード）」の説明です。回路資源（バス・演算器・ポートなど）の奪い合いによって生じる停止です。",
          "analogy": "工場に溶接機が1台しかないのに2人が同時に使おうとする状態が構造ハザードです。資源不足が原因なので、ハードウェアを増設することで解消できます。"
        }
      },
      {
        "id": "d",
        "text": "キャッシュミスが発生し、主記憶からデータを読み込む間パイプライン全体が停止する状況。",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "キャッシュミスによるストールは「メモリレイテンシ起因のストール」です。広義にはデータハザードの一種と見ることもありますが、試験文脈でのデータハザードは前の命令の演算結果への依存（RAW 依存）を指します。",
          "analogy": "必要な本が本棚（キャッシュ）になく倉庫（主記憶）まで取りに行く待ち時間と、前工程の作業完了待ちは原因が異なります。"
        }
      }
    ],
    "overallExplanation": {
      "summary": "パイプラインハザードには構造ハザード・データハザード・制御ハザードの3種類がある。データハザードは前命令の演算結果が確定する前に後続命令が参照しようとする依存関係（主にRAW）が原因。",
      "keyPoint": "構造=資源競合、データ=演算結果の依存（RAW）、制御=分岐先未確定",
      "relatedTopics": ["パイプラインのステージ", "フォワーディング", "制御ハザードと分岐予測", "スーパースカラ"],
      "studyTip": "3種のハザードを「S・D・C（Structure, Data, Control）」と略して覚え、それぞれの対策とセットで暗記するとよいです。"
    },
    "tags": ["パイプライン", "データハザード", "RAW依存", "フォワーディング", "ハザード"]
  },
  {
    "questionId": "q-cc-027",
    "examType": "科目A",
    "category": "テクノロジ系",
    "subcategory": "computer-components",
    "topic": "キャッシュの書き込み方式",
    "level": 6,
    "question": "キャッシュメモリのライトバック方式に関する説明として正しいものはどれか。",
    "choices": [
      {
        "id": "a",
        "text": "CPU がキャッシュに書き込むと同時に、主記憶にも書き込む方式で、常にキャッシュと主記憶の内容が一致する。",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "これはライトスルー方式の説明です。書き込みのたびに主記憶へもアクセスするため、常に一致が保たれますが、書き込みのたびに主記憶への遅いアクセスが発生します。",
          "analogy": "ノートに書きながら黒板にも同時に書き続けるのがライトスルーです。二度手間ですが黒板は常に最新状態です。"
        }
      },
      {
        "id": "b",
        "text": "CPU はキャッシュにのみ書き込み、キャッシュのデータが追い出されるときに初めて主記憶へ書き戻す方式。",
        "isCorrect": true,
        "explanation": {
          "whyCorrect": "ライトバック方式では、CPUはキャッシュにのみ書き込みます。書き換えられたキャッシュラインには「ダーティビット（変更フラグ）」が立ち、そのラインが別のデータに置き換えられる（追い出される）ときにはじめて主記憶へ書き戻します。書き込み速度が速く主記憶へのアクセス回数が少なくなりますが、キャッシュと主記憶の内容が一時的に異なるため、マルチプロセッサ環境ではキャッシュコヒーレンシの管理が必要になります。",
          "analogy": "ノートにメモしておき、授業が終わってノートを別のページに使い始めるときだけ黒板に転記するイメージです。普段は黒板を消さずに済むのでスピーディです。",
          "deepDive": "ダーティビットが 1 のキャッシュラインが追い出されるとき（エビクション時）に主記憶への書き戻し（ライトバック）が行われます。読み取り専用のラインはダーティビットが 0 のままなので追い出し時の書き戻しは不要です。マルチコア CPU では各コアのキャッシュ間の一貫性を保つ「MESI プロトコル」などのキャッシュコヒーレンシプロトコルが使われます。"
        }
      },
      {
        "id": "c",
        "text": "書き込みが発生したとき、キャッシュを経由せず直接主記憶に書き込む方式で、キャッシュは読み取り専用になる。",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "これはライトアラウンド（ノーライトアロケート）方式に近い説明です。キャッシュを読み取り専用にする一般的な方式は存在しませんし、ライトバックとは全く異なります。",
          "analogy": "キャッシュをスルーして直接主記憶に書く方式は、高速なメモ帳を使わず最初から遠い倉庫に書きに行くようなもので、キャッシュの恩恵を受けられません。"
        }
      },
      {
        "id": "d",
        "text": "書き込みミスが発生したとき、主記憶からデータをキャッシュに読み込んでから書き込む方式。",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "これはライトアロケート（write-allocate）の説明であり、ライトバックとは別の概念です。ライトアロケートはライトバックと組み合わせて使われることが多いですが、それ自体がライトバック方式の定義ではありません。",
          "analogy": "ライトアロケートは「書く前にまずメモ帳にページを確保する」手順のことで、ライトバック（後で黒板に転記する）とは別の話です。"
        }
      }
    ],
    "overallExplanation": {
      "summary": "ライトバックはキャッシュにのみ書き込み、追い出し時に主記憶へ書き戻す方式。高速だが一時的にキャッシュと主記憶の内容が異なる。ライトスルーは常に両方へ書く方式で常に一致が保たれる。",
      "keyPoint": "ライトバック=後で主記憶へ（速い・ダーティビット必要）、ライトスルー=同時に主記憶へ（遅い・常に一致）",
      "relatedTopics": ["ライトスルー", "ダーティビット", "キャッシュコヒーレンシ", "MESIプロトコル"],
      "studyTip": "「バック（後で）書く」=ライトバック、「スルー（通り抜けて）書く」=ライトスルー、と語感で覚えると混同しにくくなります。"
    },
    "tags": ["キャッシュ", "ライトバック", "ライトスルー", "ダーティビット", "書き込み方式"]
  },
  {
    "questionId": "q-cc-028",
    "examType": "科目A",
    "category": "テクノロジ系",
    "subcategory": "computer-components",
    "topic": "RAIDレベル詳細",
    "level": 6,
    "question": "RAID 5 の特徴として正しいものはどれか。",
    "choices": [
      {
        "id": "a",
        "text": "データを複数のディスクに分散して書き込むが冗長性はなく、1台故障するとすべてのデータが失われる。",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "これは RAID 0（ストライピング）の説明です。パフォーマンスは高いですが、1台でも故障するとデータが失われます。",
          "analogy": "1冊のノートを縦に切り裂いて2冊に分けて書くようなものです。どちらか1冊をなくすと内容が読めなくなります。"
        }
      },
      {
        "id": "b",
        "text": "同じデータを2台のディスクに書き込むミラーリングで、1台が故障してももう1台でデータを保護できる。",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "これは RAID 1（ミラーリング）の説明です。冗長性は高いですが、2台で1台分のデータしか保存できず容量効率が低いです。",
          "analogy": "大切なノートを2冊同じ内容で書いておく方法です。1冊なくしても安心ですが、ノートが2冊必要になります。"
        }
      },
      {
        "id": "c",
        "text": "パリティ情報を専用の1台のディスクに集中して格納し、他のディスクにデータを分散する。",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "これは RAID 3 または RAID 4 の説明です。パリティ専用ディスクへの書き込みが集中してボトルネックになりやすい欠点があります。",
          "analogy": "計算係が1人しかいないので、全員の計算をその1人に任せるとその人だけ忙しくなる状態です。"
        }
      },
      {
        "id": "d",
        "text": "データとパリティ情報を複数のディスクに分散して格納し、1台のディスク故障に耐えられる。",
        "isCorrect": true,
        "explanation": {
          "whyCorrect": "RAID 5 は、データと XOR 演算で生成したパリティを複数のディスクに均等に分散して格納します。1台のディスクが故障しても、残りのディスクのデータとパリティから元のデータを復元できます。最低 3 台のディスクが必要で、容量効率は (n-1)/n（n はディスク台数）です。",
          "analogy": "5人のグループでノートを分担して書き、各自の担当ページには「他4人のページの要約（パリティ）」も一部書いておきます。1人が欠席しても残り4人の内容と要約から欠席者の内容を復元できます。",
          "deepDive": "RAID 5 のパリティは XOR 演算で計算します（A XOR B XOR C = パリティP）。故障ディスクのデータは残ったディスクで XOR を取れば復元できます。RAID 6 はパリティを2つ持ち2台故障に耐えられます。書き込み時はパリティ再計算が必要なため書き込みは RAID 0 より遅くなります。"
        }
      }
    ],
    "overallExplanation": {
      "summary": "RAID 5 はデータとパリティを全ディスクに分散し、1台故障まで耐えられる構成。RAID 0 は冗長なしのストライピング、RAID 1 はミラーリング、RAID 6 は2台故障に耐えられる。",
      "keyPoint": "RAID 5 = 分散パリティ・1台故障耐性・最低3台、RAID 6 = 分散パリティ×2・2台故障耐性・最低4台",
      "relatedTopics": ["RAID 0", "RAID 1", "RAID 6", "XOR演算", "ストライピング"],
      "studyTip": "RAID レベルを表で整理し、冗長性（故障耐性台数）・最低ディスク台数・容量効率を各行に並べて比較暗記すると試験で素早く判断できます。"
    },
    "tags": ["RAID", "RAID5", "パリティ", "冗長化", "ストレージ"]
  },
  {
    "questionId": "q-cc-029",
    "examType": "科目A",
    "category": "テクノロジ系",
    "subcategory": "computer-components",
    "topic": "実効アクセス時間計算",
    "level": 6,
    "question": "キャッシュメモリのヒット率が 90%、キャッシュへのアクセス時間が 10 ナノ秒、主記憶へのアクセス時間が 100 ナノ秒であるとき、実効アクセス時間（平均アクセス時間）は何ナノ秒か。ただし、キャッシュミス時はキャッシュアクセス後に主記憶にアクセスするものとする。",
    "choices": [
      {
        "id": "a",
        "text": "19 ナノ秒",
        "isCorrect": true,
        "explanation": {
          "whyCorrect": "実効アクセス時間 = ヒット率 × キャッシュアクセス時間 + (1 - ヒット率) × (キャッシュアクセス時間 + 主記憶アクセス時間) で計算します。= 0.9 × 10 + 0.1 × (10 + 100) = 9 + 0.1 × 110 = 9 + 11 = 19 ナノ秒。",
          "analogy": "10回のうち9回は近くの棚（10秒）で見つかり、1回は遠い倉庫まで行く（まず棚確認10秒＋倉庫100秒＝110秒）。平均すると 9×10秒＋1×110秒の平均に近い計算になります。",
          "deepDive": "「ミス時はキャッシュアクセス後に主記憶アクセス」という条件に注意が必要です。条件が「ミス時はキャッシュをスキップして主記憶のみ」であれば 0.9×10 + 0.1×100 = 9 + 10 = 19 ナノ秒で同じですが、中間ステップが変わります。問題文の設定を正確に読み取ることが重要です。今回の設定では (9 + 11) = 19 ナノ秒となります。"
        }
      },
      {
        "id": "b",
        "text": "55 ナノ秒",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "55 ナノ秒は (10 + 100) / 2 = 55 のように単純平均した場合に相当します。ヒット率による重みづけを考慮していません。",
          "analogy": "近くの棚と遠い倉庫の距離を単純に足して2で割っても、実際の平均移動距離は使用頻度によって変わります。頻繁に使う棚が近ければ平均は短くなります。"
        }
      },
      {
        "id": "c",
        "text": "28 ナノ秒",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "28 ナノ秒は 0.9 × 10 + 0.1 × 100 = 9 + 10 = 19 ではなく、ミス時のキャッシュ時間を二重計算せず単純に 0.9×10 + 0.1×(100+90)÷... などの誤った計算から生じる値です。計算式を正確に適用することが必要です。",
          "analogy": "途中の計算で足し算や掛け算の順序を間違えると答えがずれます。式を丁寧に展開して計算しましょう。"
        }
      },
      {
        "id": "d",
        "text": "10 ナノ秒",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "10 ナノ秒はキャッシュアクセス時間のみを答えた場合です。ヒット率が 100% でない限り、主記憶へのアクセスが発生する分だけ実効アクセス時間はキャッシュアクセス時間より長くなります。",
          "analogy": "10回中1回は遠い倉庫まで行くのに、平均移動時間が近くの棚と同じになることはありえません。"
        }
      }
    ],
    "overallExplanation": {
      "summary": "実効アクセス時間 = ヒット率 × キャッシュ時間 + (1 - ヒット率) × (キャッシュ時間 + 主記憶時間)。ミス時の条件（キャッシュアクセス後に主記憶か、直接主記憶か）を問題文で確認すること。",
      "keyPoint": "実効アクセス時間 = h×Tc + (1-h)×(Tc+Tm)。今回は 0.9×10 + 0.1×110 = 19 ナノ秒。",
      "relatedTopics": ["キャッシュヒット率", "ライトバック・ライトスルー", "メモリ階層", "アクセス時間"],
      "studyTip": "「ミス時にキャッシュもアクセスするか否か」で式が変わります。問題文の条件を必ず確認してから式を立てる習慣をつけましょう。"
    },
    "tags": ["実効アクセス時間", "キャッシュ", "ヒット率", "計算問題", "メモリ階層"]
  },
  {
    "questionId": "q-cc-030",
    "examType": "科目A",
    "category": "テクノロジ系",
    "subcategory": "computer-components",
    "topic": "半導体メモリの種類と特性",
    "level": 6,
    "question": "SRAM と DRAM の比較として正しい記述はどれか。",
    "choices": [
      {
        "id": "a",
        "text": "SRAM は DRAM よりも集積度が高く、大容量の主記憶に適している。",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "SRAM は 1 ビットあたり 4〜6 個のトランジスタが必要なため、DRAM（1 トランジスタ＋1 コンデンサ）と比較して集積度が低く、大容量化やコスト面で不利です。主記憶には安価で高密度な DRAM が使われます。",
          "analogy": "SRAM は個室にしっかりした本棚（複数トランジスタ）、DRAM はコンテナに電荷を蓄える簡素な構造です。個室は高品質ですが場所を取ります。"
        }
      },
      {
        "id": "b",
        "text": "DRAM はリフレッシュ動作が不要なため、SRAM より消費電力が低い。",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "リフレッシュが不要なのは SRAM です。DRAM はコンデンサの電荷が自然に漏れるため定期的なリフレッシュ（再書き込み）が必要であり、その分の電力も消費します。",
          "analogy": "DRAM は水漏れするバケツを使うようなもので、定期的に水を補充（リフレッシュ）しなければなりません。SRAM は水漏れしない密閉容器なので補充不要です。"
        }
      },
      {
        "id": "c",
        "text": "SRAM は DRAM よりもアクセス速度が速く、主にキャッシュメモリに使用される。",
        "isCorrect": true,
        "explanation": {
          "whyCorrect": "SRAM はフリップフロップ回路（複数トランジスタ）で構成され、リフレッシュ不要で高速アクセスが可能です。一方 DRAM はコンデンサで電荷を保持するためリフレッシュが必要で低速です。そのためアクセス速度が重要なキャッシュメモリには SRAM、大容量が必要な主記憶には DRAM が使われます。",
          "analogy": "SRAM は電気で即座に動くしっかりしたスイッチ群（フリップフロップ）で、DRAM は「充電」を保ちながら動く電池のようなものです。充電型は速くありませんが沢山並べられます。",
          "deepDive": "SRAM の 1 セルは 6 トランジスタ（6T-SRAM）が一般的で、電源さえあれば情報を保持します（揮発性だが、リフレッシュ不要）。DRAM は 1 トランジスタ＋1 コンデンサで構成され、コンデンサの電荷が数十ミリ秒以内に漏れるため定期リフレッシュが必要です。フラッシュメモリは不揮発性（電源を切っても保持）で SSD や USB メモリに使われますが、書き換え回数に上限があります。"
        }
      },
      {
        "id": "d",
        "text": "フラッシュメモリは揮発性で電源を切るとデータが消えるため、主記憶の補助として使用される。",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "フラッシュメモリは不揮発性メモリで、電源を切ってもデータが保持されます。そのため SSD・USB メモリ・スマートフォンのストレージなどに広く使われています。揮発性なのは SRAM や DRAM です。",
          "analogy": "フラッシュメモリは「書き込んだら消えない黒板」のようなもので、電気を切っても内容が残ります。DRAM は電源を切ると消える「電光掲示板」のようなものです。"
        }
      }
    ],
    "overallExplanation": {
      "summary": "SRAM は高速・リフレッシュ不要・高コスト・低密度でキャッシュに使用。DRAM は低速・リフレッシュ必要・低コスト・高密度で主記憶に使用。フラッシュメモリは不揮発性でストレージに使用。",
      "keyPoint": "SRAM=速い・リフレッシュ不要・キャッシュ向き、DRAM=遅い・リフレッシュ必要・主記憶向き",
      "relatedTopics": ["キャッシュメモリ", "主記憶", "フラッシュメモリ", "不揮発性メモリ", "メモリ階層"],
      "studyTip": "「S（Static）は静的＝安定していてリフレッシュ不要」「D（Dynamic）は動的＝電荷が動いて（漏れて）リフレッシュ必要」と語源から覚えると理解しやすいです。"
    },
    "tags": ["SRAM", "DRAM", "フラッシュメモリ", "半導体メモリ", "キャッシュ"]
  },
  {
    "questionId": "q-cc-031",
    "examType": "科目A",
    "category": "テクノロジ系",
    "subcategory": "computer-components",
    "topic": "マイクロプログラム制御",
    "level": 7,
    "question": "マイクロプログラム制御方式に関する説明として正しいものはどれか。",
    "choices": [
      {
        "id": "a",
        "text": "命令デコーダをハードウェアの論理回路のみで構成し、制御信号を高速に生成する方式。",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "これはハードワイヤード制御（組み合わせ論理回路による制御）の説明です。回路が固定のため変更には再設計が必要ですが、処理速度は速いです。",
          "analogy": "電気配線を直接つなぎ合わせて機械を動かす方式です。速いですが、動作を変えたいときは配線をやり直す必要があります。"
        }
      },
      {
        "id": "b",
        "text": "機械語命令をマイクロ命令のシーケンスに変換して実行する方式で、命令セットの追加・変更が容易。",
        "isCorrect": true,
        "explanation": {
          "whyCorrect": "マイクロプログラム制御では、各機械語命令（マクロ命令）に対応するマイクロ命令の列（マイクロプログラム）をコントロールストア（ROM）に格納します。命令フェッチ後、対応するマイクロプログラムを逐次実行して制御信号を生成します。ファームウェアの書き換えで命令セットを変更できる柔軟性が特徴ですが、ハードワイヤードに比べて速度は劣ります。",
          "analogy": "楽譜（マイクロプログラム）を読みながら演奏する演奏家のようなものです。楽譜を差し替えれば別の曲（別の命令動作）が演奏できます。直接配線（ハードワイヤード）は暗譜した演奏家で速いですが変更が困難です。",
          "deepDive": "コントロールストア（制御記憶）は ROM または書き換え可能な RAM で構成されます。マイクロ命令には水平型（並列度が高く高速だがビット幅大）と垂直型（ビット幅小だがデコードが必要で低速）があります。CISC プロセッサはマイクロプログラム制御が多く、RISC プロセッサはハードワイヤード制御が主流です。"
        }
      },
      {
        "id": "c",
        "text": "プロセッサが命令を投機的に先読みして実行し、不要だった場合は結果を破棄する実行方式。",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "これは投機的実行（Speculative Execution）の説明です。マイクロプログラム制御とは全く別の概念です。",
          "analogy": "「たぶんこっちの道だろう」と先に進んでみて、違ったら戻る運転方法が投機的実行です。マイクロプログラム制御は命令の解釈方法に関する話です。"
        }
      },
      {
        "id": "d",
        "text": "複数の演算器を並列に動作させ、1クロックで複数の命令を同時に発行する方式。",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "これはスーパースカラ方式の説明です。複数の実行ユニットに命令を並列発行して処理能力を高める方式で、マイクロプログラム制御とは別です。",
          "analogy": "レジが複数あって同時に複数の客を会計するスーパーマーケットのイメージがスーパースカラです。マイクロプログラム制御はレジの中の処理手順書に関する話です。"
        }
      }
    ],
    "overallExplanation": {
      "summary": "マイクロプログラム制御は機械語命令をコントロールストアに格納されたマイクロ命令列で実行する方式。柔軟性が高く命令セット変更が容易だが、ハードワイヤード制御より低速。",
      "keyPoint": "マイクロプログラム制御=コントロールストア（ROM）＋柔軟・低速、ハードワイヤード=固定論理回路・高速・変更困難",
      "relatedTopics": ["ハードワイヤード制御", "CISC", "RISC", "コントロールストア", "マイクロ命令"],
      "studyTip": "「マイクロプログラム＝変更できるプログラムで制御＝柔軟」「ハードワイヤード＝配線で制御＝速いが固定」という対比で覚えましょう。"
    },
    "tags": ["マイクロプログラム制御", "ハードワイヤード制御", "CISC", "コントロールストア", "CPU制御"]
  },
  {
    "questionId": "q-cc-032",
    "examType": "科目A",
    "category": "テクノロジ系",
    "subcategory": "computer-components",
    "topic": "投機的実行",
    "level": 7,
    "question": "プロセッサの投機的実行（Speculative Execution）に関する説明として正しいものはどれか。",
    "choices": [
      {
        "id": "a",
        "text": "プロセッサが命令を順序通りに実行し、途中で分岐があっても次の命令の取り出しを待機する方式。",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "これは投機的実行をしない場合（順序実行・分岐ストール）の説明です。分岐命令の確定を待ってから次の命令を取り出すため、パイプラインが止まります。",
          "analogy": "交差点で「次どちらへ曲がるか」の指示を待ってから動き出す慎重な運転方法です。安全ですが時間がかかります。"
        }
      },
      {
        "id": "b",
        "text": "分岐先を予測して先行して命令を実行し、予測が外れた場合はその結果を破棄してやり直す方式。",
        "isCorrect": true,
        "explanation": {
          "whyCorrect": "投機的実行では、分岐予測器が分岐先を予測し、その予測に基づいて先行して命令をフェッチ・デコード・実行します。予測が正しければ大幅な時間短縮になります。予測が外れた場合はパイプラインをフラッシュ（投機的に実行した結果を破棄）してやり直します。現代の高性能プロセッサはほぼすべてこの方式を採用しており、予測精度は 95% 以上に達することも多いです。なお 2018 年に明らかになった Spectre 脆弱性は投機的実行の副作用を利用したものです。",
          "analogy": "「たぶん右折する」と予想して右の車線に入りながら走り続け、本当に右折なら時間を節約でき、左折だったら元の車線に戻る運転方法です。大半の場合は予想が当たって効率的です。",
          "deepDive": "分岐予測には静的予測（常に taken や not-taken を予測）と動的予測（過去の分岐履歴を参照する BTB: Branch Target Buffer や 2-bit 飽和カウンタ）があります。投機的に実行した命令がレジスタや主記憶に結果を書き込む前に「リオーダバッファ（ROB）」で保持され、予測ミス時は ROB をフラッシュします。Spectre・Meltdown はこの投機的実行とキャッシュのサイドチャネルを組み合わせた脆弱性です。"
        }
      },
      {
        "id": "c",
        "text": "複数のスレッドを同時に実行し、あるスレッドが待機中に別のスレッドのパイプラインを利用する方式。",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "これはハイパースレッディング（Intel の SMT: Simultaneous Multi-Threading）の説明です。投機的実行とは別の技術です。",
          "analogy": "複数の作業を交互に切り替えながら効率よくこなすマルチタスク作業に相当します。投機的実行は1つの作業を先読みして進める話です。"
        }
      },
      {
        "id": "d",
        "text": "命令を発行した順序とは異なる順序で完了させることで、データ依存のある命令間の待ちを隠す方式。",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "これはアウトオブオーダー実行（Out-of-Order Execution）の説明です。投機的実行と組み合わせて使われることが多いですが、別の概念です。",
          "analogy": "料理の手順を「並行できるところから先に進める」方法がアウトオブオーダーです。投機的実行は「たぶん次に必要なもの」を先に準備する話です。"
        }
      }
    ],
    "overallExplanation": {
      "summary": "投機的実行は分岐先を予測して先行実行し、予測ミス時に結果を破棄してやり直す技術。現代の高性能プロセッサに広く採用されており、Spectre 脆弱性の根本原因でもある。",
      "keyPoint": "分岐予測 → 先行実行 → 予測的中なら採用・外れたらフラッシュ",
      "relatedTopics": ["分岐予測", "パイプライン", "アウトオブオーダー実行", "Spectre脆弱性", "BTB"],
      "studyTip": "「投機（スペキュレーション）＝賭けに出て先に動く」と覚え、予測外れ時のペナルティ（パイプラインフラッシュ）とセットで理解しましょう。"
    },
    "tags": ["投機的実行", "分岐予測", "パイプライン", "Spectre", "アウトオブオーダー"]
  },
  {
    "questionId": "q-cc-033",
    "examType": "科目A",
    "category": "テクノロジ系",
    "subcategory": "computer-components",
    "topic": "NUMAアーキテクチャ",
    "level": 7,
    "question": "NUMA（Non-Uniform Memory Access）アーキテクチャの特徴として正しいものはどれか。",
    "choices": [
      {
        "id": "a",
        "text": "すべてのプロセッサが均等な速度で同一のメモリにアクセスでき、スケーラビリティに優れる。",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "これは UMA（Uniform Memory Access）、つまり SMP（Symmetric Multi-Processing）の特徴です。全プロセッサが同一の共有メモリバスを使うため、プロセッサ数が増えるとバスがボトルネックになります。",
          "analogy": "全員が同じ1つの共有の棚から取り出す方式です。人数が増えると棚が混雑してスピードが落ちます。"
        }
      },
      {
        "id": "b",
        "text": "各プロセッサはローカルメモリを持ち、ローカルへのアクセスは高速だが、他プロセッサのメモリへのアクセスは低速になる。",
        "isCorrect": true,
        "explanation": {
          "whyCorrect": "NUMA では各プロセッサ（またはプロセッサグループ）に隣接したローカルメモリが割り当てられます。ローカルメモリへのアクセスは高速ですが、別のプロセッサのローカルメモリ（リモートメモリ）へのアクセスはインターコネクト（例：AMD EPYC の Infinity Fabric、Intel の QPI/UPI）を経由するため低速になります。この非均等性（Non-Uniform）が名前の由来です。大規模なマルチソケットサーバに多く使われます。",
          "analogy": "各社員が自分のデスク（ローカルメモリ）を持っているオフィスのようなものです。自分のデスクの書類はすぐ取れますが、別のフロアの社員の机から書類を借りる（リモートアクセス）と時間がかかります。",
          "deepDive": "NUMA 対応の OS スケジューラはプロセスをローカルメモリに近いノードで実行するよう努めます（NUMA Awareness）。Linux では numactl コマンドや libnuma ライブラリでプロセスの NUMA ノードを制御できます。NUMA の性能を最大化するには、データをアクセスするプロセッサと同じノードに配置することが重要です。"
        }
      },
      {
        "id": "c",
        "text": "プロセッサとメモリを1チップ上に統合し、チップ外へのメモリアクセスをなくすことで均等な高速アクセスを実現する。",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "これは SoC（System on Chip）や HBM（High Bandwidth Memory）のコンセプトに近い説明です。NUMA のアーキテクチャとは異なります。",
          "analogy": "電話・テレビ・冷蔵庫をすべて1つの筐体に詰め込む統合型家電のイメージです。NUMA は複数の家電が通信しながら協調する分散型の仕組みです。"
        }
      },
      {
        "id": "d",
        "text": "メモリアクセスを完全に分散させ、どのプロセッサからも同じ時間でアクセスできるよう設計されたクラスタ構成。",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "NUMA は Non-Uniform（非均等）なアクセス特性を持つ設計であり、均等なアクセス時間を目指す設計ではありません。均等アクセスを実現しようとするのは UMA の考え方です。",
          "analogy": "どこの席に座っても同じ距離の食堂（UMA）に対して、自分の席に近いキッチンは速いが遠いキッチンは時間がかかる食堂（NUMA）です。"
        }
      }
    ],
    "overallExplanation": {
      "summary": "NUMA はプロセッサごとにローカルメモリを持ち、ローカルアクセスは高速・リモートアクセスは低速という非均等な特性を持つアーキテクチャ。大規模マルチソケットサーバに使われる。",
      "keyPoint": "NUMA = 非均等アクセス（ローカル速い・リモート遅い）、UMA = 均等アクセス（全プロセッサ同速度）",
      "relatedTopics": ["UMA（SMP）", "マルチプロセッサ", "NUMA Awareness", "Infinity Fabric", "スケーラビリティ"],
      "studyTip": "「Non-Uniform（非均等）」の意味を軸に、ローカルとリモートのアクセス速度の違いを意識して覚えましょう。"
    },
    "tags": ["NUMA", "マルチプロセッサ", "メモリアクセス", "UMA", "サーバアーキテクチャ"]
  },
  {
    "questionId": "q-cc-034",
    "examType": "科目A",
    "category": "テクノロジ系",
    "subcategory": "computer-components",
    "topic": "メモリインタリーブの効果計算",
    "level": 7,
    "question": "主記憶を 4 バンクのインタリーブ方式で構成した場合に期待できる効果はどれか。ただし、各バンクのアクセスサイクルは 100 ナノ秒、インタリーブを行わない場合の連続アクセス時間も 100 ナノ秒/ワードとする。",
    "choices": [
      {
        "id": "a",
        "text": "アクセス時間は変わらないが、主記憶の容量が 4 倍になる。",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "インタリーブはバンクを並列動作させるスループット向上技術であり、容量を増やすものではありません。容量はバンク数×各バンク容量ですが、目的はスループット改善です。",
          "analogy": "レジを4台並べることでお客の処理が速くなりますが、店の売り場面積が広くなるわけではありません。"
        }
      },
      {
        "id": "b",
        "text": "連続するアドレスへのアクセスにおいて、スループットが最大 4 倍程度に向上する。",
        "isCorrect": true,
        "explanation": {
          "whyCorrect": "メモリインタリーブは主記憶を複数バンクに分割し、連続アドレスが隣接するバンクに分散するよう配置します。バンク0へのアクセス中にバンク1・2・3へのアクセス要求を先行発行できるため、連続アクセスのスループットを最大でバンク数倍（今回は最大4倍）に向上させられます。1ワードあたりのアクセス時間そのものは短縮されませんが、単位時間当たりに転送できるデータ量（スループット）が増加します。",
          "analogy": "4つのレジが並んでいるスーパーのイメージです。1台のレジが1人の客を処理している間に他の3台も別の客を処理しているので、店全体でのお客の処理数は4倍に近づきます。",
          "deepDive": "インタリーブによるスループット向上は「連続アドレスアクセス」に効果的で、ランダムアクセスでは改善が限定的です。4バンクの場合、アドレス mod 4 でバンクを決定します。バンクサイクルタイムを T とすると、理想的な連続アクセス時のスループットは 1ワード/（T/4）= 4倍になります。実際にはバンク数と一致しない場合やアクセスパターンにより効果は低下します。"
        }
      },
      {
        "id": "c",
        "text": "1 ワードあたりのアクセス時間が 100 ナノ秒から 25 ナノ秒に短縮される。",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "インタリーブは1ワードの「レイテンシ（アクセス時間）」を短縮するものではなく、連続アクセスの「スループット」を向上させる技術です。1バンクへのアクセスに 100 ナノ秒かかることは変わりません。",
          "analogy": "4台のレジを並べても、1人の客を処理する時間（レイテンシ）は変わりません。並列処理で単位時間の処理人数（スループット）が増えるのです。"
        }
      },
      {
        "id": "d",
        "text": "メモリのリフレッシュ頻度が 1/4 に減り、リフレッシュによる性能低下が軽減される。",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "インタリーブはバンクを並列動作させる技術であり、DRAM のリフレッシュ頻度を制御する仕組みとは別物です。リフレッシュはバンク数に関係なく各バンクで個別に行われます。",
          "analogy": "レジを4台並べても、各レジが定期メンテナンスをする頻度は変わりません。メンテナンスとレジの台数は別の問題です。"
        }
      }
    ],
    "overallExplanation": {
      "summary": "メモリインタリーブは複数バンクを並列動作させ、連続アドレスアクセスのスループットをバンク数倍まで向上させる技術。レイテンシ短縮ではなくスループット向上が目的。",
      "keyPoint": "インタリーブ = スループット向上（最大バンク数倍）、レイテンシ（1ワードのアクセス時間）は変わらない",
      "relatedTopics": ["メモリバンク", "DRAM", "キャッシュ", "メモリ帯域幅", "連続アクセス"],
      "studyTip": "「レイテンシ（1回の待ち時間）」と「スループット（単位時間の処理量）」を混同しないよう意識しましょう。インタリーブはスループット改善であることを押さえておきます。"
    },
    "tags": ["メモリインタリーブ", "スループット", "メモリバンク", "連続アクセス", "主記憶"]
  },
  {
    "questionId": "q-cc-035",
    "examType": "科目A",
    "category": "テクノロジ系",
    "subcategory": "computer-components",
    "topic": "GPUコンピューティング",
    "level": 7,
    "question": "GPU（Graphics Processing Unit）をコンピューティングに活用する GPGPU の説明として正しいものはどれか。",
    "choices": [
      {
        "id": "a",
        "text": "GPU は CPU と比較して、コア数は少ないが1コアあたりの演算能力が高く、複雑な分岐処理に優れる。",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "GPU はコア数が多い（数千〜数万）のが特徴です。ただし1コアあたりの能力は低く、複雑な分岐処理には不向きです。GPU の強みは大量の単純演算の並列実行です。",
          "analogy": "CPU は少数精鋭の専門職チーム（少コア・高性能）、GPU は大人数の単純作業チーム（多コア・低性能）です。複雑な案件は専門職、単純な仕分け作業は大人数チームが向いています。"
        }
      },
      {
        "id": "b",
        "text": "GPU は複雑な制御フローや分岐処理が得意なため、オペレーティングシステムの中核処理に活用される。",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "GPU は複雑な分岐制御には不向きで、OS のような制御重視の処理には向いていません。GPU の強みは同じ演算を大量のデータに並列適用するSIMD的な処理です。",
          "analogy": "大人数の単純作業チームに、臨機応変な判断が次々と必要な管理業務をやらせても生産性は上がりません。"
        }
      },
      {
        "id": "c",
        "text": "大量のコアで同じ演算を多数のデータに並列実行するため、機械学習や科学技術計算に活用される。",
        "isCorrect": true,
        "explanation": {
          "whyCorrect": "GPU はもともと 3D グラフィックスの大量の頂点・ピクセル演算を並列処理するために設計されました。この「多数のコアで同じ演算を大量データに適用する」特性（SIMD: Single Instruction Multiple Data 的な処理）が、ディープラーニングの行列演算・シミュレーション・物理演算などにも適しており、GPGPU（General-Purpose computing on GPU）として汎用計算に活用されています。NVIDIA の CUDA や OpenCL などのフレームワークで GPU を利用できます。",
          "analogy": "同じダンスの振り付けを 1,000 人が一斉に踊るダンスチームのようなものです。全員が同じ動作を同時に行う場合は圧倒的な効率を発揮します。これが行列乗算などの並列演算に最適な理由です。",
          "deepDive": "GPU の演算器群（CUDA コアなど）は Warp（32 スレッドの束）単位で同一命令を実行します（SIMT: Single Instruction Multiple Thread）。スレッドによって分岐の向きが異なる場合は Warp Divergence が発生して効率が落ちます。ディープラーニングでの主要演算である行列積（GEMM）は分岐が少なく大量の乗算・加算であるため GPU に最適です。"
        }
      },
      {
        "id": "d",
        "text": "GPU は専用のメモリを持たず CPU のメインメモリを共有するため、データ転送のオーバーヘッドがない。",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "ディスクリート GPU（外付けグラフィックカード）は VRAM と呼ばれる専用のメモリを持ちます。CPU の主記憶と GPU の VRAM 間でデータを転送するオーバーヘッドが存在し、これが GPGPU の課題の1つです。なお、CPU と GPU がメモリを共有するユニファイドメモリアーキテクチャ（Apple Silicon など）は例外です。",
          "analogy": "工場の加工ラインが別棟にある場合、材料を運んで加工して戻す往復の時間（転送オーバーヘッド）がかかります。同じ建物内にあれば（ユニファイドメモリ）その往復が省けます。"
        }
      }
    ],
    "overallExplanation": {
      "summary": "GPGPU は GPU の多数のコアで同一演算を大量データに並列実行する特性を活用する技術。機械学習・科学技術計算・シミュレーションなどに広く使われる。CUDA・OpenCL などのフレームワークで利用可能。",
      "keyPoint": "GPU = 多コア・並列演算得意（SIMT）・分岐苦手。GPGPU = GPU を機械学習や科学計算に活用",
      "relatedTopics": ["CUDA", "OpenCL", "SIMD", "機械学習", "行列演算", "VRAM"],
      "studyTip": "「GPU = グラフィック用の並列処理マシン」→「その並列性を一般計算に使う = GPGPU」という流れで覚えましょう。"
    },
    "tags": ["GPU", "GPGPU", "並列演算", "CUDA", "機械学習", "SIMT"]
  },
  {
    "questionId": "q-cc-036",
    "examType": "科目A",
    "category": "テクノロジ系",
    "subcategory": "computer-components",
    "topic": "キャッシュの連想度",
    "level": 8,
    "question": "セットアソシアティブキャッシュ（Set-Associative Cache）の特徴を説明したものとして正しいものはどれか。",
    "choices": [
      {
        "id": "a",
        "text": "主記憶の各ラインが格納できるキャッシュスロットが1つだけに決まる方式で、ハードウェアが単純で高速に検索できる。",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "これはダイレクトマップキャッシュの説明です。インデックスによって格納先が1つに決まるため回路がシンプルですが、同じインデックスのデータが競合するとミス率が上がる欠点があります。",
          "analogy": "図書館で本の分類番号が棚の位置を一意に決める方式です。同じ番号の本が2冊来たら1冊は棚に入れられません。"
        }
      },
      {
        "id": "b",
        "text": "主記憶のどのラインでも任意のキャッシュスロットに格納でき、ミス率が最小になるが検索回路が複雑になる。",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "これはフルアソシアティブキャッシュの説明です。完全連想型ともいい、キャッシュ全スロットを並列に検索するため回路が複雑・高コストになりますが、ミス率は最低になります。",
          "analogy": "図書館のどの棚にも本を自由に置ける方式です。柔軟ですが「どこに置いたか」を全棚から探す必要があります。"
        }
      },
      {
        "id": "c",
        "text": "キャッシュをいくつかのセットに分割し、主記憶の各ラインが特定のセット内の任意のスロットに格納できる方式。",
        "isCorrect": true,
        "explanation": {
          "whyCorrect": "セットアソシアティブキャッシュは、ダイレクトマップとフルアソシアティブの中間の方式です。キャッシュをN個のスロットからなるセットに分割し、主記憶のラインは「アドレス mod セット数」でセットが決まり、そのセット内のN個のスロット（N-way）のいずれかに格納できます。N=1 がダイレクトマップ、N=全スロット数がフルアソシアティブに相当します。実用的なプロセッサでは 4-way・8-way・16-way などが多く使われます。",
          "analogy": "図書館の棚をグループ分けし（例：A〜Mと N〜Z の2グループ）、本の頭文字でグループは決まるが、グループ内ではどの棚にも置けるイメージです。完全自由より制約があり、完全固定より柔軟です。",
          "deepDive": "N-way セットアソシアティブの場合、1セット内のN個のスロットのどれを置き換えるかを決める「置き換えポリシー（LRU, FIFO, Random など）」が必要です。アドレスのビット構成は「タグ | セットインデックス | ブロックオフセット」です。セット数=C/（N×ブロックサイズ）で計算されます。ダイレクトマップはスラッシング（競合ミス）が多く、フルアソシアティブはハードウェアが高コストなため、実用上はセットアソシアティブが最も多く使われます。"
        }
      },
      {
        "id": "d",
        "text": "主記憶のページをキャッシュにマッピングする際、ページテーブルを用いて変換するためTLBが必要になる。",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "これは仮想記憶のページングとTLBに関する説明であり、キャッシュの連想度（アソシアティビティ）とは別の概念です。キャッシュの連想度はキャッシュライン配置の柔軟性に関する話です。",
          "analogy": "住所（仮想アドレス）を実際の場所（物理アドレス）に変換する地図（ページテーブル・TLB）の話は、図書館の棚の配置方式（キャッシュ連想度）とは全く異なる問題です。"
        }
      }
    ],
    "overallExplanation": {
      "summary": "セットアソシアティブキャッシュはダイレクトマップとフルアソシアティブの中間方式。セットでアドレスを限定し、セット内は自由に配置できる。N-way と表記し実用的な高性能プロセッサで広く採用されている。",
      "keyPoint": "ダイレクトマップ=1スロット固定・単純・競合多い、セットアソシアティブ=Nスロットから選択・中間、フルアソシアティブ=全スロット自由・ミス少・複雑",
      "relatedTopics": ["ダイレクトマップキャッシュ", "フルアソシアティブキャッシュ", "LRU置き換え", "キャッシュミス率", "タグビット"],
      "studyTip": "「1(ダイレクト)・N(セットアソシアティブ)・全(フルアソシアティブ)」の順でアソシアティビティが高くなり、ミス率は下がるがコストが上がると覚えましょう。"
    },
    "tags": ["セットアソシアティブ", "ダイレクトマップ", "フルアソシアティブ", "キャッシュ連想度", "N-way"]
  },
  {
    "questionId": "q-cc-037",
    "examType": "科目A",
    "category": "テクノロジ系",
    "subcategory": "computer-components",
    "topic": "ページ置換アルゴリズム",
    "level": 8,
    "question": "仮想記憶のページ置換アルゴリズムにおいて、LRU（Least Recently Used）方式の説明として正しいものはどれか。",
    "choices": [
      {
        "id": "a",
        "text": "最も長い時間にわたってアクセスされていないページを置き換える方式で、参照の局所性を利用し、実用的なアルゴリズムとして広く使われる。",
        "isCorrect": true,
        "explanation": {
          "whyCorrect": "LRU は「最近最も使われていないページを追い出す」方式です。参照の局所性（最近使われたものは近い将来も使われやすい）に基づいており、OPT（最適）アルゴリズムに次いでページフォルト率が低いとされています。完全な LRU の実装にはアクセス時刻の記録が必要でオーバーヘッドが大きいため、実際の OS では近似アルゴリズム（クロックアルゴリズムなど）が使われることが多いです。",
          "analogy": "本棚がいっぱいになったとき「一番長い間開いていない本」を取り出してしまう方法です。最近よく読む本ほど棚に残り続けます。",
          "deepDive": "LRU の近似実装として「クロックアルゴリズム（Second-Chance アルゴリズム）」があります。各ページに参照ビット（1ビット）を持ち、時計の針が一周しながら参照ビットが 0 のページを置き換えます。参照ビットが 1 のページは 0 にリセットしてチャンスを与えます。完全な LRU よりオーバーヘッドが少なく実用的です。FIFO 方式はベラディの異常（フレーム数を増やすとページフォルトが増える現象）が起きることがありますが、LRU では起きません。"
        }
      },
      {
        "id": "b",
        "text": "ページをキューで管理し、最初にロードされたページを最初に追い出す方式で、実装が簡単だがベラディの異常が発生することがある。",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "これは FIFO（First In First Out）方式の説明です。実装は簡単ですが、古くロードされていても頻繁に使われているページが追い出されることがあり、またベラディの異常が発生します。",
          "analogy": "お店の在庫管理で「先に入荷したものを先に売る」方式です。賞味期限がある食品には合理的ですが、ページは古くても有用なものがあります。"
        }
      },
      {
        "id": "c",
        "text": "将来最も長い間アクセスされないページを予知して置き換える方式で、最もページフォルト率が低いが実現不可能。",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "これは OPT（最適アルゴリズム、Bélády's Algorithm）の説明です。将来のアクセスパターンを知っていると仮定して最適な選択をしますが、実際には未来を予知できないため実装できません。",
          "analogy": "「未来の天気予報が100%正確」という前提の傘の持ち方です。理論的に最善ですが現実には不可能です。"
        }
      },
      {
        "id": "d",
        "text": "ページのアクセス頻度をカウントし、最もカウント数が少ないページを置き換える方式。",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "これは LFU（Least Frequently Used）方式の説明です。アクセス頻度が最小のページを追い出す方法で、LRU とは判断基準が異なります（LRU は「最近のアクセス」、LFU は「累計アクセス頻度」）。",
          "analogy": "「今まで一番少ない回数しか読んでいない本」を捨てるのが LFU です。LRU は「最後に読んだのが一番昔の本」を捨てます。"
        }
      }
    ],
    "overallExplanation": {
      "summary": "LRU は最近最も使われていないページを置き換える方式。参照の局所性を利用し実用的。OPT が最善だが実現不可能。FIFO は簡単だがベラディの異常あり。LFU は頻度ベース。",
      "keyPoint": "LRU=最近使っていないページを追い出す・局所性を活用・実用的、OPT=最適だが未来予知が必要、FIFO=単純・ベラディの異常あり",
      "relatedTopics": ["FIFO", "OPT", "LFU", "クロックアルゴリズム", "ベラディの異常", "参照の局所性"],
      "studyTip": "「LRU = Least Recently Used = 最近使っていないものを追い出す」と英語の意味から覚えると LFU（最も使っていない＝頻度）との区別がしやすくなります。"
    },
    "tags": ["LRU", "ページ置換", "仮想記憶", "FIFO", "OPT", "ページフォルト"]
  },
  {
    "questionId": "q-cc-038",
    "examType": "科目A",
    "category": "テクノロジ系",
    "subcategory": "computer-components",
    "topic": "パイプラインのスループット計算",
    "level": 8,
    "question": "5 ステージのパイプラインプロセッサで、各ステージの処理時間が IF=20ns、ID=15ns、EX=25ns、MEM=20ns、WB=15ns であるとき、このパイプラインの 1 クロックサイクル時間と、100 命令実行したときの合計処理時間として正しい組合せはどれか。ただし、ハザードによるストールはなく、クロックサイクルは最も遅いステージに合わせるものとする。",
    "choices": [
      {
        "id": "a",
        "text": "クロックサイクル：19ns（平均）、合計処理時間：1,900ns",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "パイプラインのクロックサイクルは各ステージの平均ではなく、最も遅いステージ（ボトルネック）に合わせます。平均では最も遅いステージでオーバーフローが発生します。",
          "analogy": "工場の流れ作業で、最も時間がかかる工程に全体のペースを合わせないと、その工程に仕掛品が積み上がります。"
        }
      },
      {
        "id": "b",
        "text": "クロックサイクル：25ns、合計処理時間：2,600ns",
        "isCorrect": true,
        "explanation": {
          "whyCorrect": "クロックサイクル = 最長ステージ時間 = EX = 25ns です。N 命令をパイプラインで実行するときの合計時間 = (ステージ数 + N - 1) × クロックサイクル = (5 + 100 - 1) × 25ns = 104 × 25ns = 2,600ns です。パイプラインが満杯になるまでに最初の命令が全ステージを通過する（ステージ数-1の遅延）ことを考慮します。",
          "analogy": "5段の流れ作業で、1個目の製品が完成するまでに5工程分かかりますが、それ以降は最も遅い工程（25ns）ごとに1個完成します。100個完成させるには（5-1）個分の立ち上がり＋100個分＝104サイクル×25nsです。",
          "deepDive": "パイプライン実行の合計時間の公式は T = (k + N - 1) × Δt です。k=ステージ数、N=命令数、Δt=クロックサイクル（最長ステージ）。非パイプラインの場合は N × (各ステージの合計) = 100 × (20+15+25+20+15) = 100 × 95ns = 9,500ns です。パイプライン化でスループットは約 95ns/25ns ≒ 3.8 倍向上しています。"
        }
      },
      {
        "id": "c",
        "text": "クロックサイクル：25ns、合計処理時間：9,500ns",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "9,500ns は非パイプライン（シリアル実行）の場合の計算です。100命令 × (20+15+25+20+15)ns = 100 × 95ns = 9,500ns です。パイプラインでは命令がオーバーラップして実行されるため、はるかに高速になります。",
          "analogy": "流れ作業を使わずに製品を1つ1つ順番に完成させていく場合の時間です。流れ作業（パイプライン）の方がずっと速くなります。"
        }
      },
      {
        "id": "d",
        "text": "クロックサイクル：25ns、合計処理時間：2,500ns",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "2,500ns は 100 × 25ns の計算で、パイプラインの立ち上がり（最初の命令が全ステージを通過するまでの遅延：ステージ数-1サイクル）を無視した値です。正確には (5-1) サイクル分の追加が必要で (100+4) × 25 = 2,600ns が正解です。",
          "analogy": "流れ作業が「満杯」になる前の立ち上がり時間を忘れた計算です。最初の数個の製品が完成するまでは工程が埋まっておらず、単純に100倍ではありません。"
        }
      }
    ],
    "overallExplanation": {
      "summary": "パイプラインのクロックサイクルは最長ステージ時間。N命令の合計時間は (k+N-1)×Δt（k=ステージ数）。非パイプラインに対して大幅なスループット向上が得られる。",
      "keyPoint": "クロックサイクル = 最長ステージ（ボトルネック）。N命令合計 = (k+N-1)×Δt",
      "relatedTopics": ["パイプラインステージ", "ハザード", "スループット", "CPI", "パイプライン効率"],
      "studyTip": "「(ステージ数 + 命令数 - 1) × クロックサイクル」という公式を何度も計算して体に染み込ませましょう。立ち上がり分の +k-1 を忘れないことが重要です。"
    },
    "tags": ["パイプライン", "スループット計算", "クロックサイクル", "ステージ数", "実行時間"]
  },
  {
    "questionId": "q-cc-039",
    "examType": "科目A",
    "category": "テクノロジ系",
    "subcategory": "computer-components",
    "topic": "バス調停方式",
    "level": 8,
    "question": "バスアービトレーション（バス調停）の「デイジーチェーン方式」の説明として正しいものはどれか。",
    "choices": [
      {
        "id": "a",
        "text": "各デバイスが乱数によってバスの使用権を獲得する確率が決まり、衝突した場合は一定時間待機して再試行する方式。",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "これは CSMA/CD（イーサネット）などで使われる衝突回避方式の説明に近いものです。バス調停のデイジーチェーン方式とは異なります。",
          "analogy": "じゃんけんで順番を決める方式のようなものです。デイジーチェーンは「花輪のように順番につながった」ものです。"
        }
      },
      {
        "id": "b",
        "text": "バス使用要求を一元的に管理するアービタが各デバイスの優先度に基づいてバス使用権を付与する集中型方式。",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "これは集中型アービトレーションの一般的な説明です。デイジーチェーン方式も集中型の一種ですが、この説明はプログラム制御方式やポーリング方式により近いです。デイジーチェーン方式の特有の仕組み（バスグラント信号の伝播順）が含まれていません。",
          "analogy": "本部（アービタ）が各部署から使用申請を受けて順番を割り当てる方式です。デイジーチェーンはこの伝達の仕組みが特殊なのです。"
        }
      },
      {
        "id": "c",
        "text": "バスグラント信号がデバイスを直列につながった順に伝わり、要求を出しているデバイスがグラント信号を止めてバスを確保する方式で、接続順が優先度になる。",
        "isCorrect": true,
        "explanation": {
          "whyCorrect": "デイジーチェーン方式では、アービタがバスグラント（BG: Bus Grant）信号を送出し、この信号がデバイス1→デバイス2→デバイス3→…と直列（チェーン状）に伝わります。バスを要求しているデバイスはグラント信号を自分の次のデバイスへ流さずに止め、バスの使用権を取得します。バスリクエスト（BR: Bus Request）線は全デバイスがワイヤードOR でアービタに接続されています。デバイスの接続順（チェーンの先の方）が優先度になり、先のデバイスが要求を出していれば後のデバイスはグラント信号を受け取れません。",
          "analogy": "回覧板（バスグラント信号）が席を順番に回ってきて、急ぎの用事がある人（バス要求中のデバイス）がそこで回覧板を止めて使う方式です。前の席の人が優先で、後ろの席は前の席が終わるまで使えません。",
          "deepDive": "デイジーチェーン方式の利点は配線が少ない点（BR 線1本・BG 線1本）ですが、欠点として接続順で優先度が固定されることと、先のデバイスが常にバスを占有すると後のデバイスがスタベーション（永続的な権利獲得不能）に陥ることです。これに対しポーリング方式はアービタが各デバイスをカウンタで順次ポーリングして公平性を保ちます。独立要求方式（Independent Request）は各デバイスが個別の BR・BG 線を持ち高速ですが配線数が増えます。"
        }
      },
      {
        "id": "d",
        "text": "複数のデバイスが同時にバス要求を出した場合に、すべての要求を無効化して一定時間後に再申請させる方式。",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "このような方式はバス調停方式として一般的ではなく、デイジーチェーン方式の説明でもありません。デイジーチェーンでは同時要求があっても、グラント信号の伝播先で優先度に従って解決されます。",
          "analogy": "全員の手が同時に挙がったら「全員やり直し」となる方式は非効率で一般的な調停方式ではありません。"
        }
      }
    ],
    "overallExplanation": {
      "summary": "デイジーチェーン方式はバスグラント信号がデバイス列を直列に伝わり、要求中のデバイスが信号を止めてバスを確保する方式。接続順が固定の優先度になる。",
      "keyPoint": "デイジーチェーン = グラント信号が数珠つなぎ伝播 → 要求デバイスで停止 → バス確保。接続順 = 優先度。",
      "relatedTopics": ["バスアービトレーション", "ポーリング方式", "独立要求方式", "バスプロトコル", "スタベーション"],
      "studyTip": "「デイジーチェーン = 花輪・数珠つなぎ」というイメージで、グラント信号がチェーンを順番に流れる様子を思い浮かべると理解しやすくなります。"
    },
    "tags": ["バス調停", "デイジーチェーン", "バスアービトレーション", "バスグラント", "優先度"]
  },
  {
    "questionId": "q-cc-040",
    "examType": "科目A",
    "category": "テクノロジ系",
    "subcategory": "computer-components",
    "topic": "メモリの誤り訂正（ECC）",
    "level": 8,
    "question": "ECC（Error Correcting Code）メモリに関する記述として正しいものはどれか。",
    "choices": [
      {
        "id": "a",
        "text": "ECC メモリはデータビットのみを保持し、誤りを検出した場合はシステムを即座にシャットダウンして安全を確保する。",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "ECC メモリは誤り訂正ビット（チェックビット）も合わせて保持しており、システムをシャットダウンせずに自動的に誤りを訂正します。シャットダウンするのは誤り検出（ECD）のみの装置やセキュリティ設計の場合です。",
          "analogy": "ECC はすり傷に絆創膏を貼って自動治療するシステムです。シャットダウン（入院）する必要はありません。軽微な誤りは自分で直します。"
        }
      },
      {
        "id": "b",
        "text": "データを2重に書き込んで比較することで誤りを検出する方式で、RAID 1 と同じ原理を用いている。",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "これはデュプレックス方式や TMR（Triple Modular Redundancy）の考え方に近いものです。ECC はハミング符号などの数学的な誤り訂正符号を使用しており、データを単純に2重書きするものではありません。",
          "analogy": "ECC は「誤り訂正の数式（チェックサム）」を使います。2冊同じノートを書いて見比べる方式（ミラーリング）とは根本的に仕組みが違います。"
        }
      },
      {
        "id": "c",
        "text": "ハミング符号などの誤り訂正符号を用いて、1ビットの誤りを自動的に訂正し、2ビットの誤りを検出できる。",
        "isCorrect": true,
        "explanation": {
          "whyCorrect": "ECC メモリは主にハミング符号（またはその拡張）を用いて、データビットに冗長な検査ビット（チェックビット）を付加します。SEC-DED（Single Error Correction, Double Error Detection）符号が代表的で、1ビットの誤りは自動訂正し、2ビットの誤りは検出（ただし訂正は不可）できます。サーバや医療機器・金融システムなど高信頼性が求められる環境で使用されます。64ビットデータに対し通常 8 ビットのチェックビットが付加された 72 ビット構成（72bit ECC）が一般的です。",
          "analogy": "文章の各段落の先頭文字を集めると元の文章のキーワードが読める「折り句」のようなものです。1文字が変わっても全体からどの文字が正しいか推測できます（訂正）。2文字変わると「何かおかしい」とわかりますが、どちらが正しいかはわかりません（検出のみ）。",
          "deepDive": "ハミング距離とは、2つの符号語の間で異なるビット数のことです。SEC-DED 符号はハミング距離 4 以上を確保します。チェックビットの数を r とすると、2^r ≥ データビット数 + r + 1 の条件を満たす最小の r がチェックビット数になります。64ビットデータには r = 7（2^7 = 128 ≥ 72）ビットが必要ですが、実用上は 8 ビットが使われます。ECC によりビットフリップ（α線・宇宙線による SEU: Single Event Upset）を自動訂正でき、サーバの信頼性が大幅に向上します。"
        }
      },
      {
        "id": "d",
        "text": "ECC メモリは通常の主記憶と比べてアクセス速度が大幅に速く、キャッシュメモリとして使用されることが多い。",
        "isCorrect": false,
        "explanation": {
          "whyWrong": "ECC メモリはチェックビットの計算と検証のオーバーヘッドがあるため、通常の DRAM よりわずかに遅くなります。キャッシュに使用されることはなく、主記憶（主に信頼性が求められるサーバの主記憶）として使用されます。",
          "analogy": "ECC メモリは「答え合わせ付きのノート」です。答え合わせに少し時間がかかるので、超高速のキャッシュ（棚）ではなく、信頼性が重要な主記憶（書庫）に使います。"
        }
      }
    ],
    "overallExplanation": {
      "summary": "ECC メモリはハミング符号などを用いて1ビット誤りを自動訂正・2ビット誤りを検出できる（SEC-DED）。サーバ・医療・金融などの高信頼性が必要なシステムに使用される。",
      "keyPoint": "ECC = SEC-DED（1ビット自動訂正・2ビット検出）・ハミング符号・サーバ向け",
      "relatedTopics": ["ハミング符号", "SEC-DED", "パリティビット", "SEU", "高信頼性システム"],
      "studyTip": "「パリティ = 1ビット検出のみ」「ECC = 1ビット訂正 + 2ビット検出（SEC-DED）」という対比で覚え、それぞれ何ビット余分に必要かもセットで理解しましょう。"
    },
    "tags": ["ECC", "ハミング符号", "SEC-DED", "誤り訂正", "メモリ信頼性", "パリティ"]
  }
]
